{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from scripts.ml_method import *\n",
    "from scripts.proj1_helpers import *\n",
    "from scripts.preprocess import *\n",
    "from scripts.split_data import *\n",
    "from scripts.model_testing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ind = load_csv_data('higgs-data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data and standarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stand_x = standardize(x1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(stand_x, y, 0.8, myseed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algortihm for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses, w = logistic_regression(y_tr, x_tr , np.zeros(stand_x.shape[1]),500, 0.000005)\n",
    "# losses, w = reg_logistic_regression(y_tr, x_tr , np.zeros(stand_x.shape[1]),300, 0.000005, 0.1)\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# clf1 = AdaBoostClassifier(\n",
    "#     DecisionTreeClassifier(max_depth=2),\n",
    "# #     n_estimators=600,\n",
    "#     learning_rate=1)\n",
    "\n",
    "# clf = clf1.fit(X = x_tr, y = y_tr)\n",
    "# clf.score(x_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74275999999999998"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, x_te)\n",
    "# acc = y_pred == y_te\n",
    "\n",
    "np.mean(y_pred == y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_error, test_error\n",
      "0.0001 , 0.68 , 0.478715871587\n",
      "0.0002 , 0.72 , 0.650010002\n",
      "0.0003 , 0.693333333333 , 0.654216264879\n",
      "0.0004 , 0.67 , 0.654305722289\n",
      "0.0005 , 0.688 , 0.658125062531\n",
      "0.0006 , 0.68 , 0.671298779268\n",
      "0.0007 , 0.657142857143 , 0.679031321925\n",
      "0.0008 , 0.675 , 0.682337870296\n",
      "0.0009 , 0.684444444444 , 0.681060954859\n",
      "0.001 , 0.68 , 0.677701701702\n",
      "0.0011 , 0.690909090909 , 0.679567524277\n",
      "0.0012 , 0.69 , 0.683928714457\n",
      "0.0013 , 0.686153846154 , 0.685735456093\n",
      "0.0014 , 0.7 , 0.691852593631\n",
      "0.0015 , 0.696 , 0.692610916375\n",
      "0.0016 , 0.695 , 0.694322916667\n",
      "0.0017 , 0.691764705882 , 0.693474907342\n",
      "0.0018 , 0.695555555556 , 0.695720296534\n",
      "0.0019 , 0.692631578947 , 0.696327021341\n",
      "0.002 , 0.688 , 0.697675350701\n",
      "0.0021 , 0.681904761905 , 0.699905802185\n",
      "0.0022 , 0.68 , 0.702878332331\n",
      "0.0023 , 0.685217391304 , 0.70427984364\n",
      "0.0024 , 0.68 , 0.706992782678\n",
      "0.0025 , 0.688 , 0.70942556391\n",
      "0.0026 , 0.692307692308 , 0.710174453579\n",
      "0.0027 , 0.688888888889 , 0.711685550988\n",
      "0.0028 , 0.691428571429 , 0.712747693542\n",
      "0.0029 , 0.699310344828 , 0.7120690001\n",
      "0.003 , 0.701333333333 , 0.714194583751\n",
      "0.0031 , 0.707096774194 , 0.713812819741\n",
      "0.0032 , 0.70625 , 0.715774478331\n",
      "0.0033 , 0.706666666667 , 0.715180094311\n",
      "0.0034 , 0.707058823529 , 0.715745534818\n",
      "0.0035 , 0.712 , 0.716311088811\n",
      "0.0036 , 0.715555555556 , 0.717318346046\n",
      "0.0037 , 0.715675675676 , 0.716499046472\n",
      "0.0038 , 0.711578947368 , 0.71723750251\n",
      "0.0039 , 0.714871794872 , 0.715968276277\n",
      "0.004 , 0.709 , 0.717550200803\n",
      "0.0041 , 0.712195121951 , 0.717927502761\n",
      "0.0042 , 0.712380952381 , 0.718232576823\n",
      "0.0043 , 0.710697674419 , 0.719204579693\n",
      "0.0044 , 0.706363636364 , 0.717774206509\n",
      "0.0045 , 0.705777777778 , 0.717693621296\n",
      "0.0046 , 0.70347826087 , 0.717661241712\n",
      "0.0047 , 0.703829787234 , 0.7168130212\n",
      "0.0048 , 0.7075 , 0.716808681672\n",
      "0.0049 , 0.70693877551 , 0.716607376143\n",
      "0.005 , 0.708 , 0.715863316583\n",
      "0.0051 , 0.709019607843 , 0.715883003317\n",
      "0.0052 , 0.708461538462 , 0.715954965822\n",
      "0.0053 , 0.710188679245 , 0.716175731376\n",
      "0.0054 , 0.712592592593 , 0.715704805952\n",
      "0.0055 , 0.712727272727 , 0.715463046757\n",
      "0.0056 , 0.711428571429 , 0.716749798874\n",
      "0.0057 , 0.713684210526 , 0.717212109021\n",
      "0.0058 , 0.713103448276 , 0.718008449004\n",
      "0.0059 , 0.717288135593 , 0.717859370285\n",
      "0.006 , 0.716 , 0.719158953722\n",
      "0.0061 , 0.71606557377 , 0.720390381326\n",
      "0.0062 , 0.715483870968 , 0.720708392031\n",
      "0.0063 , 0.714285714286 , 0.720853376271\n",
      "0.0064 , 0.71625 , 0.720483091787\n",
      "0.0065 , 0.716923076923 , 0.720881731253\n",
      "0.0066 , 0.717575757576 , 0.720801288504\n",
      "0.0067 , 0.714626865672 , 0.720902043693\n",
      "0.0068 , 0.715882352941 , 0.721727748691\n",
      "0.0069 , 0.717101449275 , 0.721284865572\n",
      "0.007 , 0.717714285714 , 0.720410876133\n",
      "0.0071 , 0.717183098592 , 0.720652633699\n",
      "0.0072 , 0.716666666667 , 0.722493956487\n",
      "0.0073 , 0.719452054795 , 0.721490883449\n",
      "0.0074 , 0.720540540541 , 0.721281482974\n",
      "0.0075 , 0.720533333333 , 0.720519899244\n",
      "0.0076 , 0.721578947368 , 0.72012898025\n",
      "0.0077 , 0.718441558442 , 0.720842487151\n",
      "0.0078 , 0.718974358974 , 0.721036081435\n",
      "0.0079 , 0.719493670886 , 0.720931357726\n",
      "0.008 , 0.72 , 0.720435483871\n",
      "0.0081 , 0.71950617284 , 0.720608932352\n",
      "0.0082 , 0.716935090288 , 0.720533492505\n",
      "0.0083 , 0.717108433735 , 0.721036603812\n",
      "0.0084 , 0.715238095238 , 0.721867688584\n",
      "0.0085 , 0.717176470588 , 0.721524962179\n",
      "0.0086 , 0.717209302326 , 0.720730280412\n",
      "0.0087 , 0.718620689655 , 0.721190356098\n",
      "0.0088 , 0.717272727273 , 0.720891848265\n",
      "0.0089 , 0.721348314607 , 0.72154172132\n",
      "0.009 , 0.721333333333 , 0.721263370333\n",
      "0.0091 , 0.718681318681 , 0.721485518216\n",
      "0.0092 , 0.719565217391 , 0.721453371013\n",
      "0.0093 , 0.718279569892 , 0.721538306248\n",
      "0.0094 , 0.71914893617 , 0.721445588532\n",
      "0.0095 , 0.719578947368 , 0.721772841999\n",
      "0.0096 , 0.719166666667 , 0.721595315024\n",
      "0.0097 , 0.719587628866 , 0.721688377259\n",
      "0.0098 , 0.720408163265 , 0.722528782064\n",
      "0.0099 , 0.71948261924 , 0.722259479812\n"
     ]
    }
   ],
   "source": [
    "# curve_lambda(y, stand_x, 0.8, 1e-5, 1e-5, 1e-4,)\n",
    "curve_size_train_set(y, stand_x, 0.0001, 0.0001, 0.01)\n",
    "\n",
    "# x_tr, x_te, y_tr, y_te = split_data(stand_x, y, 0.1, myseed=1)\n",
    "# losses, w = logistic_regression(y_tr, x_tr , np.zeros(stand_x.shape[1]),300, 0.000005)\n",
    "\n",
    "# y1, x1, ind1 = load_csv_data('higgs-data/test.csv')\n",
    "# stand_x1 = standardize(x1, True)\n",
    "\n",
    "# y_pred1 = predict_labels(w, stand_x1)\n",
    "\n",
    "# create_csv_submission(ind1, y_pred1, 'prediction.csv')\n",
    "\n",
    "#     print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/299): loss=138629.43611198498, w0=-0.157815, w1=0.005289413002084378\n",
      "Gradient Descent(1/299): loss=122098.04916753742, w0=-0.27608604694776895, w1=0.008451036560543912\n",
      "Gradient Descent(2/299): loss=115318.37299877546, w0=-0.369436604472764, w1=0.010471228307359023\n",
      "Gradient Descent(3/299): loss=111746.76032441092, w0=-0.4437373352715086, w1=0.009807271136105428\n",
      "Gradient Descent(4/299): loss=109531.51235037915, w0=-0.503749683989031, w1=0.007656634431041442\n",
      "Gradient Descent(5/299): loss=108049.22874542566, w0=-0.5527583129719168, w1=0.004592727558575985\n",
      "Gradient Descent(6/299): loss=107003.38187956886, w0=-0.5931408413009783, w1=0.0010269370614764718\n",
      "Gradient Descent(7/299): loss=106233.59648473878, w0=-0.6266767799963137, w1=-0.0027963342073413413\n",
      "Gradient Descent(8/299): loss=105646.2346965372, w0=-0.6547197930607975, w1=-0.006726197855023011\n",
      "Gradient Descent(9/299): loss=105183.70103623677, w0=-0.6783169752845356, w1=-0.01067004097304507\n",
      "Gradient Descent(10/299): loss=104809.15385141138, w0=-0.6982881040187726, w1=-0.01457052908949163\n",
      "Gradient Descent(11/299): loss=104498.28005221364, w0=-0.7152817142536771, w1=-0.01839206336876914\n",
      "Gradient Descent(12/299): loss=104234.61083602815, w0=-0.7298154477765965, w1=-0.02211260376592727\n",
      "Gradient Descent(13/299): loss=104006.7353251496, w0=-0.7423057170038494, w1=-0.02571870859380961\n",
      "Gradient Descent(14/299): loss=103806.58491454957, w0=-0.7530898524656604, w1=-0.029202484433687236\n",
      "Gradient Descent(15/299): loss=103628.34665129094, w0=-0.7624428572212933, w1=-0.03255968241981641\n",
      "Gradient Descent(16/299): loss=103467.75849134919, w0=-0.7705902232229613, w1=-0.03578848891713412\n",
      "Gradient Descent(17/299): loss=103321.64279153291, w0=-0.777717830767908, w1=-0.03888874172228191\n",
      "Gradient Descent(18/299): loss=103187.59190473403, w0=-0.7839796610471308, w1=-0.04186141064625544\n",
      "Gradient Descent(19/299): loss=103063.75285580434, w0=-0.789503851819482, w1=-0.04470824519898984\n",
      "Gradient Descent(20/299): loss=102948.6777122214, w0=-0.794397486110629, w1=-0.04743153019026164\n",
      "Gradient Descent(21/299): loss=102841.21820918415, w0=-0.7987504040081341, w1=-0.05003391288240133\n",
      "Gradient Descent(22/299): loss=102740.45061659861, w0=-0.8026382555031164, w1=-0.05251827902337583\n",
      "Gradient Descent(23/299): loss=102645.62154407831, w0=-0.80612495959253, w1=-0.05488766329709101\n",
      "Gradient Descent(24/299): loss=102556.10841694899, w0=-0.8092646958824535, w1=-0.05714518464165991\n",
      "Gradient Descent(25/299): loss=102471.39034586918, w0=-0.8121035258557016, w1=-0.05929399984080349\n",
      "Gradient Descent(26/299): loss=102391.0264352487, w0=-0.8146807190875347, w1=-0.06133727060812149\n",
      "Gradient Descent(27/299): loss=102314.63946677867, w0=-0.8170298431029475, w1=-0.0632781405572981\n",
      "Gradient Descent(28/299): loss=102241.9035024009, w0=-0.8191796629028933, w1=-0.0651197192753879\n",
      "Gradient Descent(29/299): loss=102172.53437065755, w0=-0.8211548864572024, w1=-0.06686507134933953\n",
      "Gradient Descent(30/299): loss=102106.2822929056, w0=-0.8229767849460179, w1=-0.068517208713064\n",
      "Gradient Descent(31/299): loss=102042.92611162612, w0=-0.8246637106945144, w1=-0.07007908511283603\n",
      "Gradient Descent(32/299): loss=101982.26872883082, w0=-0.8262315311877918, w1=-0.07155359184146304\n",
      "Gradient Descent(33/299): loss=101924.1334664761, w0=-0.8276939939730117, w1=-0.07294355417058497\n",
      "Gradient Descent(34/299): loss=101868.36113520218, w0=-0.8290630344253569, w1=-0.07425172812263607\n",
      "Gradient Descent(35/299): loss=101814.80765115158, w0=-0.830349036098437, w1=-0.07548079738122984\n",
      "Gradient Descent(36/299): loss=101763.34207903614, w0=-0.8315610515638612, w1=-0.07663337025662444\n",
      "Gradient Descent(37/299): loss=101713.84500722245, w0=-0.832706990165473, w1=-0.07771197671836716\n",
      "Gradient Descent(38/299): loss=101666.20718051426, w0=-0.8337937778931922, w1=-0.07871906559513484\n",
      "Gradient Descent(39/299): loss=101620.32833100585, w0=-0.8348274935647677, w1=-0.07965700213124183\n",
      "Gradient Descent(40/299): loss=101576.11615906295, w0=-0.8358134846600201, w1=-0.08052806617889494\n",
      "Gradient Descent(41/299): loss=101533.48542718185, w0=-0.8367564654752069, w1=-0.08133445137905705\n",
      "Gradient Descent(42/299): loss=101492.357140854, w0=-0.8376605997706824, w1=-0.08207826571060457\n",
      "Gradient Descent(43/299): loss=101452.65780328755, w0=-0.838529569797419, w1=-0.08276153372869852\n",
      "Gradient Descent(44/299): loss=101414.31874385236, w0=-0.8393666335139117, w1=-0.08338620064172989\n",
      "Gradient Descent(45/299): loss=101377.27553035873, w0=-0.8401746719017357, w1=-0.08395413810128254\n",
      "Gradient Descent(46/299): loss=101341.46747889853, w0=-0.8409562284470123, w1=-0.0844671512630902\n",
      "Gradient Descent(47/299): loss=101306.83726976193, w0=-0.8417135429248385, w1=-0.08492698642020013\n",
      "Gradient Descent(48/299): loss=101273.33066567639, w0=-0.8424485814735126, w1=-0.0853353384083596\n",
      "Gradient Descent(49/299): loss=101240.89631483392, w0=-0.8431630645349784, w1=-0.08569385707641415\n",
      "Gradient Descent(50/299): loss=101209.48561210465, w0=-0.8438584936441644, w1=-0.08600415236045943\n",
      "Gradient Descent(51/299): loss=101179.05259070182, w0=-0.8445361774273015, w1=-0.08626779780493618\n",
      "Gradient Descent(52/299): loss=101149.55382229926, w0=-0.8451972566715273, w1=-0.08648633263994988\n",
      "Gradient Descent(53/299): loss=101120.94831245205, w0=-0.8458427280429222, w1=-0.08666126269403211\n",
      "Gradient Descent(54/299): loss=101093.19738644645, w0=-0.8464734659608166, w1=-0.08679406048649035\n",
      "Gradient Descent(55/299): loss=101066.26456635608, w0=-0.8470902422255794, w1=-0.08688616482805522\n",
      "Gradient Descent(56/299): loss=101040.11544285087, w0=-0.8476937431671604, w1=-0.0869389801983213\n",
      "Gradient Descent(57/299): loss=101014.71754592503, w0=-0.848284584265239, w1=-0.08695387609401838\n",
      "Gradient Descent(58/299): loss=100990.04021814637, w0=-0.8488633223464278, w1=-0.08693218647276552\n",
      "Gradient Descent(59/299): loss=100966.0544930202, w0=-0.849430465571605, w1=-0.08687520936147348\n",
      "Gradient Descent(60/299): loss=100942.7329800641, w0=-0.8499864814863451, w1=-0.08678420665871811\n",
      "Gradient Descent(61/299): loss=100920.04975738029, w0=-0.8505318034279965, w1=-0.08666040413429921\n",
      "Gradient Descent(62/299): loss=100897.98027194038, w0=-0.8510668355755872, w1=-0.08650499161364641\n",
      "Gradient Descent(63/299): loss=100876.5012474301, w0=-0.8515919569041815, w1=-0.08631912332658945\n",
      "Gradient Descent(64/299): loss=100855.59059928547, w0=-0.8521075242720876, w1=-0.08610391839670173\n",
      "Gradient Descent(65/299): loss=100835.22735644775, w0=-0.8526138748334674, w1=-0.08586046144706821\n",
      "Gradient Descent(66/299): loss=100815.39158932015, w0=-0.8531113279342301, w1=-0.08558980329962916\n",
      "Gradient Descent(67/299): loss=100796.06434341238, w0=-0.8536001866177049, w1=-0.08529296174738514\n",
      "Gradient Descent(68/299): loss=100777.22757817831, w0=-0.8540807388394606, w1=-0.08497092238120624\n",
      "Gradient Descent(69/299): loss=100758.86411058746, w0=-0.8545532584679262, w1=-0.08462463945547717\n",
      "Gradient Descent(70/299): loss=100740.95756300633, w0=-0.8550180061289631, w1=-0.08425503677916955\n",
      "Gradient Descent(71/299): loss=100723.4923150047, w0=-0.8554752299377588, w1=-0.08386300862108714\n",
      "Gradient Descent(72/299): loss=100706.45345873729, w0=-0.8559251661498346, w1=-0.08344942061994884\n",
      "Gradient Descent(73/299): loss=100689.82675758525, w0=-0.8563680397540234, w1=-0.08301511069165685\n",
      "Gradient Descent(74/299): loss=100673.5986077723, w0=-0.8568040650234975, w1=-0.08256088992755337\n",
      "Gradient Descent(75/299): loss=100657.7560026991, w0=-0.8572334460358573, w1=-0.08208754347872005\n",
      "Gradient Descent(76/299): loss=100642.28649976247, w0=-0.8576563771695711, w1=-0.08159583142243813\n",
      "Gradient Descent(77/299): loss=100627.17818945192, w0=-0.8580730435813696, w1=-0.0810864896078289\n",
      "Gradient Descent(78/299): loss=100612.41966653283, w0=-0.8584836216673112, w1=-0.08056023047845033\n",
      "Gradient Descent(79/299): loss=100598.00000314508, w0=-0.8588882795089378, w1=-0.08001774387025977\n",
      "Gradient Descent(80/299): loss=100583.90872366403, w0=-0.8592871773050902, w1=-0.07945969778387711\n",
      "Gradient Descent(81/299): loss=100570.13578118096, w0=-0.8596804677894254, w1=-0.07888673913051708\n",
      "Gradient Descent(82/299): loss=100556.67153547857, w0=-0.8600682966333805, w1=-0.0782994944513141\n",
      "Gradient Descent(83/299): loss=100543.50673238402, w0=-0.8604508028341947, w1=-0.07769857061005099\n",
      "Gradient Descent(84/299): loss=100530.63248439555, w0=-0.86082811908758, w1=-0.07708455545953462\n",
      "Gradient Descent(85/299): loss=100518.04025248747, w0=-0.8612003721446728, w1=-0.07645801848204528\n",
      "Gradient Descent(86/299): loss=100505.72182900619, w0=-0.8615676831529967, w1=-0.07581951140443005\n",
      "Gradient Descent(87/299): loss=100493.66932157845, w0=-0.8619301679812728, w1=-0.07516956878852077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(88/299): loss=100481.87513795933, w0=-0.8622879375280422, w1=-0.07450870859763904\n",
      "Gradient Descent(89/299): loss=100470.33197175362, w0=-0.8626410980141819, w1=-0.07383743274000976\n",
      "Gradient Descent(90/299): loss=100459.0327889512, w0=-0.8629897512595086, w1=-0.07315622758994389\n",
      "Gradient Descent(91/299): loss=100447.97081521947, w0=-0.8633339949437701, w1=-0.0724655644876759\n",
      "Gradient Descent(92/299): loss=100437.13952390288, w0=-0.863673922852407, w1=-0.07176590021875139\n",
      "Gradient Descent(93/299): loss=100426.53262468337, w0=-0.8640096251075468, w1=-0.07105767747386184\n",
      "Gradient Descent(94/299): loss=100416.1440528568, w0=-0.8643411883847484, w1=-0.07034132529001576\n",
      "Gradient Descent(95/299): loss=100405.9679591882, w0=-0.8646686961160631, w1=-0.06961725947392045\n",
      "Gradient Descent(96/299): loss=100395.998700307, w0=-0.864992228680013, w1=-0.06888588300843043\n",
      "Gradient Descent(97/299): loss=100386.23082961008, w0=-0.8653118635791082, w1=-0.06814758644289409\n",
      "Gradient Descent(98/299): loss=100376.65908864062, w0=-0.8656276756055394, w1=-0.0674027482682044\n",
      "Gradient Descent(99/299): loss=100367.2783989139, w0=-0.8659397369956859, w1=-0.06665173527733084\n",
      "Gradient Descent(100/299): loss=100358.08385416307, w0=-0.8662481175740747, w1=-0.06589490291207965\n",
      "Gradient Descent(101/299): loss=100349.07071298068, w0=-0.8665528848874198, w1=-0.06513259559679865\n",
      "Gradient Descent(102/299): loss=100340.23439183159, w0=-0.8668541043293551, w1=-0.06436514705971133\n",
      "Gradient Descent(103/299): loss=100331.57045841671, w0=-0.8671518392564563, w1=-0.06359288064253327\n",
      "Gradient Descent(104/299): loss=100323.07462536704, w0=-0.8674461510961278, w1=-0.06281610959899278\n",
      "Gradient Descent(105/299): loss=100314.74274424899, w0=-0.8677370994469035, w1=-0.06203513738284574\n",
      "Gradient Descent(106/299): loss=100306.57079986425, w0=-0.8680247421716911, w1=-0.061250257925944906\n",
      "Gradient Descent(107/299): loss=100298.55490482686, w0=-0.8683091354844577, w1=-0.06046175590689306\n",
      "Gradient Descent(108/299): loss=100290.69129440312, w0=-0.8685903340308314, w1=-0.059669907010780524\n",
      "Gradient Descent(109/299): loss=100282.97632159884, w0=-0.8688683909630666, w1=-0.05887497818047897\n",
      "Gradient Descent(110/299): loss=100275.40645248182, w0=-0.8691433580097916, w1=-0.05807722785993569\n",
      "Gradient Descent(111/299): loss=100267.97826172605, w0=-0.8694152855409352, w1=-0.057276906229885544\n",
      "Gradient Descent(112/299): loss=100260.6884283654, w0=-0.8696842226281989, w1=-0.05647425543637254\n",
      "Gradient Descent(113/299): loss=100253.53373174724, w0=-0.8699502171014204, w1=-0.05566950981244707\n",
      "Gradient Descent(114/299): loss=100246.51104767367, w0=-0.8702133156011482, w1=-0.05486289609338163\n",
      "Gradient Descent(115/299): loss=100239.61734472177, w0=-0.8704735636277251, w1=-0.05405463362572458\n",
      "Gradient Descent(116/299): loss=100232.8496807328, w0=-0.8707310055871565, w1=-0.053244934570489094\n",
      "Gradient Descent(117/299): loss=100226.20519946222, w0=-0.8709856848340202, w1=-0.05243400410075366\n",
      "Gradient Descent(118/299): loss=100219.68112738151, w0=-0.8712376437116517, w1=-0.05162204059393024\n",
      "Gradient Descent(119/299): loss=100213.27477062517, w0=-0.8714869235898265, w1=-0.050809235818936864\n",
      "Gradient Descent(120/299): loss=100206.98351207394, w0=-0.8717335649001364, w1=-0.049995775118493534\n",
      "Gradient Descent(121/299): loss=100200.8048085687, w0=-0.8719776071692477, w1=-0.04918183758674273\n",
      "Gradient Descent(122/299): loss=100194.73618824761, w0=-0.8722190890502094, w1=-0.04836759624237982\n",
      "Gradient Descent(123/299): loss=100188.77524800085, w0=-0.8724580483519688, w1=-0.04755321819746353\n",
      "Gradient Descent(124/299): loss=100182.91965103638, w0=-0.8726945220672362, w1=-0.04673886482206179\n",
      "Gradient Descent(125/299): loss=100177.1671245519, w0=-0.8729285463988301, w1=-0.04592469190487579\n",
      "Gradient Descent(126/299): loss=100171.5154575074, w0=-0.8731601567846233, w1=-0.04511084980997205\n",
      "Gradient Descent(127/299): loss=100165.96249849277, w0=-0.8733893879211987, w1=-0.04429748362974135\n",
      "Gradient Descent(128/299): loss=100160.50615368714, w0=-0.873616273786315, w1=-0.04348473333419275\n",
      "Gradient Descent(129/299): loss=100155.14438490389, w0=-0.8738408476602739, w1=-0.042672733916681464\n",
      "Gradient Descent(130/299): loss=100149.875207718, w0=-0.8740631421462711, w1=-0.04186161553616063\n",
      "Gradient Descent(131/299): loss=100144.69668967181, w0=-0.8742831891898085, w1=-0.04105150365603944\n",
      "Gradient Descent(132/299): loss=100139.60694855446, w0=-0.874501020097235, w1=-0.040242519179723026\n",
      "Gradient Descent(133/299): loss=100134.60415075294, w0=-0.8747166655534789, w1=-0.03943477858290341\n",
      "Gradient Descent(134/299): loss=100129.68650966945, w0=-0.8749301556390312, w1=-0.038628394042665555\n",
      "Gradient Descent(135/299): loss=100124.85228420355, w0=-0.8751415198462279, w1=-0.03782347356346753\n",
      "Gradient Descent(136/299): loss=100120.09977729485, w0=-0.8753507870948823, w1=-0.03702012110005063\n",
      "Gradient Descent(137/299): loss=100115.42733452405, w0=-0.8755579857473084, w1=-0.036218436677330934\n",
      "Gradient Descent(138/299): loss=100110.83334276889, w0=-0.8757631436227737, w1=-0.03541851650732189\n",
      "Gradient Descent(139/299): loss=100106.31622891326, w0=-0.8759662880114198, w1=-0.03462045310313484\n",
      "Gradient Descent(140/299): loss=100101.87445860621, w0=-0.8761674456876796, w1=-0.03382433539010291\n",
      "Gradient Descent(141/299): loss=100097.50653506878, w0=-0.8763666429232226, w1=-0.03303024881407183\n",
      "Gradient Descent(142/299): loss=100093.21099794669, w0=-0.8765639054994562, w1=-0.03223827544690148\n",
      "Gradient Descent(143/299): loss=100088.9864222063, w0=-0.8767592587196036, w1=-0.03144849408921991\n",
      "Gradient Descent(144/299): loss=100084.83141707233, w0=-0.8769527274203853, w1=-0.030660980370472785\n",
      "Gradient Descent(145/299): loss=100080.74462500513, w0=-0.8771443359833212, w1=-0.02987580684630977\n",
      "Gradient Descent(146/299): loss=100076.72472071546, w0=-0.8773341083456735, w1=-0.029093043093350788\n",
      "Gradient Descent(147/299): loss=100072.77041021588, w0=-0.8775220680110479, w1=-0.02831275580137457\n",
      "Gradient Descent(148/299): loss=100068.88042990648, w0=-0.8777082380596679, w1=-0.027535008862972714\n",
      "Gradient Descent(149/299): loss=100065.0535456934, w0=-0.8778926411583376, w1=-0.026759863460713178\n",
      "Gradient Descent(150/299): loss=100061.28855213913, w0=-0.8780752995701063, w1=-0.025987378151857428\n",
      "Gradient Descent(151/299): loss=100057.58427164276, w0=-0.8782562351636473, w1=-0.025217608950676308\n",
      "Gradient Descent(152/299): loss=100053.93955364928, w0=-0.8784354694223626, w1=-0.02445060940841048\n",
      "Gradient Descent(153/299): loss=100050.3532738864, w0=-0.8786130234532249, w1=-0.023686430690921797\n",
      "Gradient Descent(154/299): loss=100046.82433362753, w0=-0.878788917995366, w1=-0.02292512165408245\n",
      "Gradient Descent(155/299): loss=100043.35165898078, w0=-0.8789631734284216, w1=-0.022166728916949853\n",
      "Gradient Descent(156/299): loss=100039.93420020127, w0=-0.8791358097806423, w1=-0.021411296932775003\n",
      "Gradient Descent(157/299): loss=100036.57093102767, w0=-0.8793068467367778, w1=-0.02065886805789317\n",
      "Gradient Descent(158/299): loss=100033.26084804002, w0=-0.8794763036457435, w1=-0.01990948261854574\n",
      "Gradient Descent(159/299): loss=100030.00297004, w0=-0.8796441995280756, w1=-0.01916317897568243\n",
      "Gradient Descent(160/299): loss=100026.79633745085, w0=-0.8798105530831842, w1=-0.018419993587793206\n",
      "Gradient Descent(161/299): loss=100023.64001173739, w0=-0.879975382696409, w1=-0.01767996107181946\n",
      "Gradient Descent(162/299): loss=100020.53307484492, w0=-0.8801387064458848, w1=-0.016943114262193838\n",
      "Gradient Descent(163/299): loss=100017.47462865623, w0=-0.8803005421092243, w1=-0.016209484268058048\n",
      "Gradient Descent(164/299): loss=100014.46379446608, w0=-0.8804609071700221, w1=-0.015479100528707794\n",
      "Gradient Descent(165/299): loss=100011.49971247267, w0=-0.8806198188241877, w1=-0.01475199086731356\n",
      "Gradient Descent(166/299): loss=100008.58154128476, w0=-0.8807772939861107, w1=-0.014028181542965902\n",
      "Gradient Descent(167/299): loss=100005.70845744482, w0=-0.8809333492946662, w1=-0.013307697301092908\n",
      "Gradient Descent(168/299): loss=100002.87965496688, w0=-0.881088001119063, w1=-0.01259056142229734\n",
      "Gradient Descent(169/299): loss=100000.09434488886, w0=-0.8812412655645419, w1=-0.011876795769660373\n",
      "Gradient Descent(170/299): loss=99997.3517548386, w0=-0.8813931584779253, w1=-0.01116642083455768\n",
      "Gradient Descent(171/299): loss=99994.65112861336, w0=-0.8815436954530278, w1=-0.01045945578103349\n",
      "Gradient Descent(172/299): loss=99991.99172577221, w0=-0.8816928918359267, w1=-0.009755918488776818\n",
      "Gradient Descent(173/299): loss=99989.3728212405, w0=-0.881840762730101, w1=-0.009055825594743779\n",
      "Gradient Descent(174/299): loss=99986.79370492665, w0=-0.8819873230014401, w1=-0.008359192533468453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(175/299): loss=99984.25368135044, w0=-0.8821325872831278, w1=-0.007666033576104232\n",
      "Gradient Descent(176/299): loss=99981.75206928191, w0=-0.8822765699804049, w1=-0.006976361868236342\n",
      "Gradient Descent(177/299): loss=99979.28820139145, w0=-0.8824192852752137, w1=-0.006290189466505513\n",
      "Gradient Descent(178/299): loss=99976.86142391019, w0=-0.8825607471307287, w1=-0.0056075273740813095\n",
      "Gradient Descent(179/299): loss=99974.47109629998, w0=-0.8827009692957762, w1=-0.004928385575023175\n",
      "Gradient Descent(180/299): loss=99972.11659093361, w0=-0.8828399653091464, w1=-0.004252773067565631\n",
      "Gradient Descent(181/299): loss=99969.79729278362, w0=-0.8829777485038015, w1=-0.003580697896363371\n",
      "Gradient Descent(182/299): loss=99967.51259912072, w0=-0.8831143320109823, w1=-0.0029121671837310254\n",
      "Gradient Descent(183/299): loss=99965.26191922056, w0=-0.883249728764217, w1=-0.002247187159910792\n",
      "Gradient Descent(184/299): loss=99963.04467407857, w0=-0.8833839515032338, w1=-0.001585763192400769\n",
      "Gradient Descent(185/299): loss=99960.86029613396, w0=-0.8835170127777814, w1=-0.0009278998143753811\n",
      "Gradient Descent(186/299): loss=99958.70822900017, w0=-0.8836489249513598, w1=-0.0002736007522281908\n",
      "Gradient Descent(187/299): loss=99956.58792720395, w0=-0.8837797002048632, w1=0.0003771310477331814\n",
      "Gradient Descent(188/299): loss=99954.49885593113, w0=-0.883909350540138, w1=0.0010242933934118093\n",
      "Gradient Descent(189/299): loss=99952.44049077986, w0=-0.8840378877834585, w1=0.001667884821838184\n",
      "Gradient Descent(190/299): loss=99950.41231752053, w0=-0.8841653235889223, w1=0.002307904574852574\n",
      "Gradient Descent(191/299): loss=99948.41383186221, w0=-0.8842916694417677, w1=0.0029443525755271606\n",
      "Gradient Descent(192/299): loss=99946.44453922569, w0=-0.8844169366616154, w1=0.0035772294053034942\n",
      "Gradient Descent(193/299): loss=99944.50395452254, w0=-0.8845411364056355, w1=0.004206536281821289\n",
      "Gradient Descent(194/299): loss=99942.59160194047, w0=-0.8846642796716444, w1=0.004832275037415648\n",
      "Gradient Descent(195/299): loss=99940.70701473398, w0=-0.8847863773011304, w1=0.005454448098260524\n",
      "Gradient Descent(196/299): loss=99938.84973502114, w0=-0.8849074399822118, w1=0.006073058464137162\n",
      "Gradient Descent(197/299): loss=99937.01931358574, w0=-0.8850274782525286, w1=0.006688109688806944\n",
      "Gradient Descent(198/299): loss=99935.21530968428, w0=-0.8851465025020704, w1=0.007299605860968829\n",
      "Gradient Descent(199/299): loss=99933.43729085865, w0=-0.8852645229759406, w1=0.007907551585782388\n",
      "Gradient Descent(200/299): loss=99931.68483275337, w0=-0.8853815497770601, w1=0.008511951966938039\n",
      "Gradient Descent(201/299): loss=99929.95751893785, w0=-0.8854975928688109, w1=0.009112812589256763\n",
      "Gradient Descent(202/299): loss=99928.25494073314, w0=-0.8856126620776222, w1=0.009710139501802464\n",
      "Gradient Descent(203/299): loss=99926.57669704313, w0=-0.8857267670954997, w1=0.010303939201490322\n",
      "Gradient Descent(204/299): loss=99924.9223941906, w0=-0.8858399174824988, w1=0.010894218617175662\n",
      "Gradient Descent(205/299): loss=99923.29164575665, w0=-0.8859521226691456, w1=0.011480985094207922\n",
      "Gradient Descent(206/299): loss=99921.68407242512, w0=-0.8860633919588041, w1=0.012064246379435161\n",
      "Gradient Descent(207/299): loss=99920.0993018305, w0=-0.8861737345299929, w1=0.012644010606645068\n",
      "Gradient Descent(208/299): loss=99918.53696840964, w0=-0.8862831594386515, w1=0.013220286282428875\n",
      "Gradient Descent(209/299): loss=99916.99671325742, w0=-0.886391675620358, w1=0.013793082272455041\n",
      "Gradient Descent(210/299): loss=99915.47818398618, w0=-0.8864992918924995, w1=0.014362407788140292\n",
      "Gradient Descent(211/299): loss=99913.98103458827, w0=-0.8866060169563956, w1=0.014928272373705517\n",
      "Gradient Descent(212/299): loss=99912.5049253024, w0=-0.8867118593993771, w1=0.015490685893605353\n",
      "Gradient Descent(213/299): loss=99911.0495224829, w0=-0.8868168276968199, w1=0.016049658520319667\n",
      "Gradient Descent(214/299): loss=99909.61449847283, w0=-0.8869209302141361, w1=0.0166052007224964\n",
      "Gradient Descent(215/299): loss=99908.1995314796, w0=-0.8870241752087226, w1=0.017157323253435133\n",
      "Gradient Descent(216/299): loss=99906.80430545378, w0=-0.8871265708318686, w1=0.01770603713990133\n",
      "Gradient Descent(217/299): loss=99905.42850997129, w0=-0.8872281251306233, w1=0.018251353671261447\n",
      "Gradient Descent(218/299): loss=99904.0718401178, w0=-0.8873288460496233, w1=0.01879328438892949\n",
      "Gradient Descent(219/299): loss=99902.73399637634, w0=-0.8874287414328821, w1=0.01933184107611603\n",
      "Gradient Descent(220/299): loss=99901.41468451753, w0=-0.8875278190255426, w1=0.01986703574787068\n",
      "Gradient Descent(221/299): loss=99900.1136154925, w0=-0.8876260864755919, w1=0.020398880641409822\n",
      "Gradient Descent(222/299): loss=99898.83050532802, w0=-0.887723551335541, w1=0.020927388206721166\n",
      "Gradient Descent(223/299): loss=99897.56507502457, w0=-0.887820221064069, w1=0.021452571097437353\n",
      "Gradient Descent(224/299): loss=99896.31705045662, w0=-0.8879161030276331, w1=0.02197444216197085\n",
      "Gradient Descent(225/299): loss=99895.08616227511, w0=-0.888011204502046, w1=0.022493014434902852\n",
      "Gradient Descent(226/299): loss=99893.87214581243, w0=-0.8881055326740189, w1=0.023008301128618923\n",
      "Gradient Descent(227/299): loss=99892.67474098955, w0=-0.8881990946426743, w1=0.02352031562518446\n",
      "Gradient Descent(228/299): loss=99891.4936922253, w0=-0.8882918974210263, w1=0.02402907146845343\n",
      "Gradient Descent(229/299): loss=99890.32874834765, w0=-0.8883839479374308, w1=0.02453458235640365\n",
      "Gradient Descent(230/299): loss=99889.17966250716, w0=-0.8884752530370057, w1=0.025036862133692415\n",
      "Gradient Descent(231/299): loss=99888.04619209247, w0=-0.8885658194830223, w1=0.02553592478442663\n",
      "Gradient Descent(232/299): loss=99886.9280986474, w0=-0.8886556539582672, w1=0.026031784425141053\n",
      "Gradient Descent(233/299): loss=99885.82514779028, w0=-0.8887447630663775, w1=0.026524455297979446\n",
      "Gradient Descent(234/299): loss=99884.73710913498, w0=-0.8888331533331475, w1=0.027013951764072747\n",
      "Gradient Descent(235/299): loss=99883.66375621366, w0=-0.8889208312078095, w1=0.027500288297109116\n",
      "Gradient Descent(236/299): loss=99882.60486640122, w0=-0.8890078030642881, w1=0.027983479477090397\n",
      "Gradient Descent(237/299): loss=99881.56022084173, w0=-0.889094075202429, w1=0.028463539984270265\n",
      "Gradient Descent(238/299): loss=99880.5296043761, w0=-0.8891796538492029, w1=0.028940484593268864\n",
      "Gradient Descent(239/299): loss=99879.51280547163, w0=-0.8892645451598853, w1=0.02941432816735939\n",
      "Gradient Descent(240/299): loss=99878.5096161529, w0=-0.8893487552192113, w1=0.02988508565292189\n",
      "Gradient Descent(241/299): loss=99877.51983193448, w0=-0.8894322900425086, w1=0.030352772074059727\n",
      "Gradient Descent(242/299): loss=99876.54325175472, w0=-0.8895151555768062, w1=0.030817402527374586\n",
      "Gradient Descent(243/299): loss=99875.57967791139, w0=-0.8895973577019216, w1=0.03127899217689565\n",
      "Gradient Descent(244/299): loss=99874.6289159983, w0=-0.8896789022315263, w1=0.031737556249158756\n",
      "Gradient Descent(245/299): loss=99873.69077484381, w0=-0.889759794914189, w1=0.03219311002843165\n",
      "Gradient Descent(246/299): loss=99872.76506645013, w0=-0.8898400414343985, w1=0.03264566885208155\n",
      "Gradient Descent(247/299): loss=99871.8516059342, w0=-0.8899196474135666, w1=0.033095248106081006\n",
      "Gradient Descent(248/299): loss=99870.95021146983, w0=-0.8899986184110097, w1=0.033541863220648595\n",
      "Gradient Descent(249/299): loss=99870.06070423106, w0=-0.8900769599249124, w1=0.03398552966602079\n",
      "Gradient Descent(250/299): loss=99869.18290833662, w0=-0.8901546773932708, w1=0.03442626294835156\n",
      "Gradient Descent(251/299): loss=99868.31665079584, w0=-0.8902317761948173, w1=0.03486407860573639\n",
      "Gradient Descent(252/299): loss=99867.46176145511, w0=-0.8903082616499275, w1=0.035298992204357275\n",
      "Gradient Descent(253/299): loss=99866.61807294638, w0=-0.8903841390215089, w1=0.03573101933474571\n",
      "Gradient Descent(254/299): loss=99865.78542063576, w0=-0.8904594135158722, w1=0.03616017560816041\n",
      "Gradient Descent(255/299): loss=99864.96364257383, w0=-0.8905340902835853, w1=0.036586476653076766\n",
      "Gradient Descent(256/299): loss=99864.152579447, w0=-0.8906081744203106, w1=0.037009938111785184\n",
      "Gradient Descent(257/299): loss=99863.35207452915, w0=-0.8906816709676261, w1=0.03743057563709534\n",
      "Gradient Descent(258/299): loss=99862.56197363551, w0=-0.89075458491383, w1=0.037848404889143594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(259/299): loss=99861.782125076, w0=-0.8908269211947304, w1=0.038263441532300914\n",
      "Gradient Descent(260/299): loss=99861.01237961097, w0=-0.8908986846944189, w1=0.03867570123217853\n",
      "Gradient Descent(261/299): loss=99860.25259040676, w0=-0.8909698802460294, w1=0.03908519965272888\n",
      "Gradient Descent(262/299): loss=99859.50261299268, w0=-0.8910405126324824, w1=0.039491952453439255\n",
      "Gradient Descent(263/299): loss=99858.76230521887, w0=-0.8911105865872151, w1=0.0398959752866158\n",
      "Gradient Descent(264/299): loss=99858.03152721471, w0=-0.8911801067948966, w1=0.04029728379475527\n",
      "Gradient Descent(265/299): loss=99857.31014134848, w0=-0.8912490778921304, w1=0.040695893608002576\n",
      "Gradient Descent(266/299): loss=99856.59801218737, w0=-0.891317504468143, w1=0.04109182034169144\n",
      "Gradient Descent(267/299): loss=99855.89500645878, w0=-0.8913853910654591, w1=0.04148507959396642\n",
      "Gradient Descent(268/299): loss=99855.20099301185, w0=-0.8914527421805644, w1=0.04187568694348373\n",
      "Gradient Descent(269/299): loss=99854.51584278037, w0=-0.8915195622645554, w1=0.04226365794718906\n",
      "Gradient Descent(270/299): loss=99853.83942874591, w0=-0.8915858557237776, w1=0.042649008138170255\n",
      "Gradient Descent(271/299): loss=99853.17162590187, w0=-0.8916516269204503, w1=0.04303175302358284\n",
      "Gradient Descent(272/299): loss=99852.51231121832, w0=-0.8917168801732812, w1=0.04341190808264652\n",
      "Gradient Descent(273/299): loss=99851.86136360746, w0=-0.8917816197580685, w1=0.04378948876471071\n",
      "Gradient Descent(274/299): loss=99851.2186638896, w0=-0.8918458499082919, w1=0.0441645104873874\n",
      "Gradient Descent(275/299): loss=99850.58409476021, w0=-0.8919095748156927, w1=0.04453698863474932\n",
      "Gradient Descent(276/299): loss=99849.95754075711, w0=-0.8919727986308431, w1=0.044906938555591865\n",
      "Gradient Descent(277/299): loss=99849.33888822875, w0=-0.8920355254637047, w1=0.04527437556175702\n",
      "Gradient Descent(278/299): loss=99848.72802530273, w0=-0.892097759384177, w1=0.04563931492651764\n",
      "Gradient Descent(279/299): loss=99848.12484185518, w0=-0.8921595044226358, w1=0.046001771883020355\n",
      "Gradient Descent(280/299): loss=99847.52922948076, w0=-0.8922207645704611, w1=0.04636176162278574\n",
      "Gradient Descent(281/299): loss=99846.94108146279, w0=-0.892281543780556, w1=0.046719299294264036\n",
      "Gradient Descent(282/299): loss=99846.36029274468, w0=-0.8923418459678562, w1=0.04707440000144496\n",
      "Gradient Descent(283/299): loss=99845.78675990136, w0=-0.8924016750098294, w1=0.04742707880252018\n",
      "Gradient Descent(284/299): loss=99845.2203811112, w0=-0.8924610347469667, w1=0.0477773507085971\n",
      "Gradient Descent(285/299): loss=99844.66105612909, w0=-0.8925199289832643, w1=0.04812523068246235\n",
      "Gradient Descent(286/299): loss=99844.1086862592, w0=-0.8925783614866967, w1=0.048470733637393855\n",
      "Gradient Descent(287/299): loss=99843.563174329, w0=-0.8926363359896818, w1=0.04881387443602006\n",
      "Gradient Descent(288/299): loss=99843.02442466315, w0=-0.8926938561895377, w1=0.04915466788922502\n",
      "Gradient Descent(289/299): loss=99842.49234305843, w0=-0.8927509257489301, w1=0.04949312875509809\n",
      "Gradient Descent(290/299): loss=99841.96683675869, w0=-0.8928075482963139, w1=0.04982927173792717\n",
      "Gradient Descent(291/299): loss=99841.44781443046, w0=-0.8928637274263651, w1=0.05016311148723398\n",
      "Gradient Descent(292/299): loss=99840.93518613922, w0=-0.892919466700406, w1=0.05049466259685044\n",
      "Gradient Descent(293/299): loss=99840.42886332565, w0=-0.8929747696468228, w1=0.05082393960403508\n",
      "Gradient Descent(294/299): loss=99839.92875878274, w0=-0.8930296397614756, w1=0.05115095698862817\n",
      "Gradient Descent(295/299): loss=99839.43478663312, w0=-0.8930840805081023, w1=0.05147572917224465\n",
      "Gradient Descent(296/299): loss=99838.94686230677, w0=-0.8931380953187136, w1=0.05179827051750359\n",
      "Gradient Descent(297/299): loss=99838.46490251936, w0=-0.8931916875939829, w1=0.052118595327293585\n",
      "Gradient Descent(298/299): loss=99837.98882525077, w0=-0.8932448607036292, w1=0.05243671784407266\n",
      "Gradient Descent(299/299): loss=99837.51854972406, w0=-0.893297617986792, w1=0.05275265224920184\n",
      "Gradient Descent(0/299): loss=138629.43611198498, w0=-0.157815, w1=0.005289413002084378\n",
      "Gradient Descent(1/299): loss=122098.04916753742, w0=-0.27608604694776895, w1=0.008451036560543912\n",
      "Gradient Descent(2/299): loss=115318.37299877546, w0=-0.369436604472764, w1=0.010471228307359023\n",
      "Gradient Descent(3/299): loss=111746.76032441092, w0=-0.4437373352715086, w1=0.009807271136105428\n",
      "Gradient Descent(4/299): loss=109531.51235037915, w0=-0.503749683989031, w1=0.007656634431041442\n",
      "Gradient Descent(5/299): loss=108049.22874542566, w0=-0.5527583129719168, w1=0.004592727558575985\n",
      "Gradient Descent(6/299): loss=107003.38187956886, w0=-0.5931408413009783, w1=0.0010269370614764718\n",
      "Gradient Descent(7/299): loss=106233.59648473878, w0=-0.6266767799963137, w1=-0.0027963342073413413\n",
      "Gradient Descent(8/299): loss=105646.2346965372, w0=-0.6547197930607975, w1=-0.006726197855023011\n",
      "Gradient Descent(9/299): loss=105183.70103623677, w0=-0.6783169752845356, w1=-0.01067004097304507\n",
      "Gradient Descent(10/299): loss=104809.15385141138, w0=-0.6982881040187726, w1=-0.01457052908949163\n",
      "Gradient Descent(11/299): loss=104498.28005221364, w0=-0.7152817142536771, w1=-0.01839206336876914\n",
      "Gradient Descent(12/299): loss=104234.61083602815, w0=-0.7298154477765965, w1=-0.02211260376592727\n",
      "Gradient Descent(13/299): loss=104006.7353251496, w0=-0.7423057170038494, w1=-0.02571870859380961\n",
      "Gradient Descent(14/299): loss=103806.58491454957, w0=-0.7530898524656604, w1=-0.029202484433687236\n",
      "Gradient Descent(15/299): loss=103628.34665129094, w0=-0.7624428572212933, w1=-0.03255968241981641\n",
      "Gradient Descent(16/299): loss=103467.75849134919, w0=-0.7705902232229613, w1=-0.03578848891713412\n",
      "Gradient Descent(17/299): loss=103321.64279153291, w0=-0.777717830767908, w1=-0.03888874172228191\n",
      "Gradient Descent(18/299): loss=103187.59190473403, w0=-0.7839796610471308, w1=-0.04186141064625544\n",
      "Gradient Descent(19/299): loss=103063.75285580434, w0=-0.789503851819482, w1=-0.04470824519898984\n",
      "Gradient Descent(20/299): loss=102948.6777122214, w0=-0.794397486110629, w1=-0.04743153019026164\n",
      "Gradient Descent(21/299): loss=102841.21820918415, w0=-0.7987504040081341, w1=-0.05003391288240133\n",
      "Gradient Descent(22/299): loss=102740.45061659861, w0=-0.8026382555031164, w1=-0.05251827902337583\n",
      "Gradient Descent(23/299): loss=102645.62154407831, w0=-0.80612495959253, w1=-0.05488766329709101\n",
      "Gradient Descent(24/299): loss=102556.10841694899, w0=-0.8092646958824535, w1=-0.05714518464165991\n",
      "Gradient Descent(25/299): loss=102471.39034586918, w0=-0.8121035258557016, w1=-0.05929399984080349\n",
      "Gradient Descent(26/299): loss=102391.0264352487, w0=-0.8146807190875347, w1=-0.06133727060812149\n",
      "Gradient Descent(27/299): loss=102314.63946677867, w0=-0.8170298431029475, w1=-0.0632781405572981\n",
      "Gradient Descent(28/299): loss=102241.9035024009, w0=-0.8191796629028933, w1=-0.0651197192753879\n",
      "Gradient Descent(29/299): loss=102172.53437065755, w0=-0.8211548864572024, w1=-0.06686507134933953\n",
      "Gradient Descent(30/299): loss=102106.2822929056, w0=-0.8229767849460179, w1=-0.068517208713064\n",
      "Gradient Descent(31/299): loss=102042.92611162612, w0=-0.8246637106945144, w1=-0.07007908511283603\n",
      "Gradient Descent(32/299): loss=101982.26872883082, w0=-0.8262315311877918, w1=-0.07155359184146304\n",
      "Gradient Descent(33/299): loss=101924.1334664761, w0=-0.8276939939730117, w1=-0.07294355417058497\n",
      "Gradient Descent(34/299): loss=101868.36113520218, w0=-0.8290630344253569, w1=-0.07425172812263607\n",
      "Gradient Descent(35/299): loss=101814.80765115158, w0=-0.830349036098437, w1=-0.07548079738122984\n",
      "Gradient Descent(36/299): loss=101763.34207903614, w0=-0.8315610515638612, w1=-0.07663337025662444\n",
      "Gradient Descent(37/299): loss=101713.84500722245, w0=-0.832706990165473, w1=-0.07771197671836716\n",
      "Gradient Descent(38/299): loss=101666.20718051426, w0=-0.8337937778931922, w1=-0.07871906559513484\n",
      "Gradient Descent(39/299): loss=101620.32833100585, w0=-0.8348274935647677, w1=-0.07965700213124183\n",
      "Gradient Descent(40/299): loss=101576.11615906295, w0=-0.8358134846600201, w1=-0.08052806617889494\n",
      "Gradient Descent(41/299): loss=101533.48542718185, w0=-0.8367564654752069, w1=-0.08133445137905705\n",
      "Gradient Descent(42/299): loss=101492.357140854, w0=-0.8376605997706824, w1=-0.08207826571060457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/299): loss=101452.65780328755, w0=-0.838529569797419, w1=-0.08276153372869852\n",
      "Gradient Descent(44/299): loss=101414.31874385236, w0=-0.8393666335139117, w1=-0.08338620064172989\n",
      "Gradient Descent(45/299): loss=101377.27553035873, w0=-0.8401746719017357, w1=-0.08395413810128254\n",
      "Gradient Descent(46/299): loss=101341.46747889853, w0=-0.8409562284470123, w1=-0.0844671512630902\n",
      "Gradient Descent(47/299): loss=101306.83726976193, w0=-0.8417135429248385, w1=-0.08492698642020013\n",
      "Gradient Descent(48/299): loss=101273.33066567639, w0=-0.8424485814735126, w1=-0.0853353384083596\n",
      "Gradient Descent(49/299): loss=101240.89631483392, w0=-0.8431630645349784, w1=-0.08569385707641415\n",
      "Gradient Descent(50/299): loss=101209.48561210465, w0=-0.8438584936441644, w1=-0.08600415236045943\n",
      "Gradient Descent(51/299): loss=101179.05259070182, w0=-0.8445361774273015, w1=-0.08626779780493618\n",
      "Gradient Descent(52/299): loss=101149.55382229926, w0=-0.8451972566715273, w1=-0.08648633263994988\n",
      "Gradient Descent(53/299): loss=101120.94831245205, w0=-0.8458427280429222, w1=-0.08666126269403211\n",
      "Gradient Descent(54/299): loss=101093.19738644645, w0=-0.8464734659608166, w1=-0.08679406048649035\n",
      "Gradient Descent(55/299): loss=101066.26456635608, w0=-0.8470902422255794, w1=-0.08688616482805522\n",
      "Gradient Descent(56/299): loss=101040.11544285087, w0=-0.8476937431671604, w1=-0.0869389801983213\n",
      "Gradient Descent(57/299): loss=101014.71754592503, w0=-0.848284584265239, w1=-0.08695387609401838\n",
      "Gradient Descent(58/299): loss=100990.04021814637, w0=-0.8488633223464278, w1=-0.08693218647276552\n",
      "Gradient Descent(59/299): loss=100966.0544930202, w0=-0.849430465571605, w1=-0.08687520936147348\n",
      "Gradient Descent(60/299): loss=100942.7329800641, w0=-0.8499864814863451, w1=-0.08678420665871811\n",
      "Gradient Descent(61/299): loss=100920.04975738029, w0=-0.8505318034279965, w1=-0.08666040413429921\n",
      "Gradient Descent(62/299): loss=100897.98027194038, w0=-0.8510668355755872, w1=-0.08650499161364641\n",
      "Gradient Descent(63/299): loss=100876.5012474301, w0=-0.8515919569041815, w1=-0.08631912332658945\n",
      "Gradient Descent(64/299): loss=100855.59059928547, w0=-0.8521075242720876, w1=-0.08610391839670173\n",
      "Gradient Descent(65/299): loss=100835.22735644775, w0=-0.8526138748334674, w1=-0.08586046144706821\n",
      "Gradient Descent(66/299): loss=100815.39158932015, w0=-0.8531113279342301, w1=-0.08558980329962916\n",
      "Gradient Descent(67/299): loss=100796.06434341238, w0=-0.8536001866177049, w1=-0.08529296174738514\n",
      "Gradient Descent(68/299): loss=100777.22757817831, w0=-0.8540807388394606, w1=-0.08497092238120624\n",
      "Gradient Descent(69/299): loss=100758.86411058746, w0=-0.8545532584679262, w1=-0.08462463945547717\n",
      "Gradient Descent(70/299): loss=100740.95756300633, w0=-0.8550180061289631, w1=-0.08425503677916955\n",
      "Gradient Descent(71/299): loss=100723.4923150047, w0=-0.8554752299377588, w1=-0.08386300862108714\n",
      "Gradient Descent(72/299): loss=100706.45345873729, w0=-0.8559251661498346, w1=-0.08344942061994884\n",
      "Gradient Descent(73/299): loss=100689.82675758525, w0=-0.8563680397540234, w1=-0.08301511069165685\n",
      "Gradient Descent(74/299): loss=100673.5986077723, w0=-0.8568040650234975, w1=-0.08256088992755337\n",
      "Gradient Descent(75/299): loss=100657.7560026991, w0=-0.8572334460358573, w1=-0.08208754347872005\n",
      "Gradient Descent(76/299): loss=100642.28649976247, w0=-0.8576563771695711, w1=-0.08159583142243813\n",
      "Gradient Descent(77/299): loss=100627.17818945192, w0=-0.8580730435813696, w1=-0.0810864896078289\n",
      "Gradient Descent(78/299): loss=100612.41966653283, w0=-0.8584836216673112, w1=-0.08056023047845033\n",
      "Gradient Descent(79/299): loss=100598.00000314508, w0=-0.8588882795089378, w1=-0.08001774387025977\n",
      "Gradient Descent(80/299): loss=100583.90872366403, w0=-0.8592871773050902, w1=-0.07945969778387711\n",
      "Gradient Descent(81/299): loss=100570.13578118096, w0=-0.8596804677894254, w1=-0.07888673913051708\n",
      "Gradient Descent(82/299): loss=100556.67153547857, w0=-0.8600682966333805, w1=-0.0782994944513141\n",
      "Gradient Descent(83/299): loss=100543.50673238402, w0=-0.8604508028341947, w1=-0.07769857061005099\n",
      "Gradient Descent(84/299): loss=100530.63248439555, w0=-0.86082811908758, w1=-0.07708455545953462\n",
      "Gradient Descent(85/299): loss=100518.04025248747, w0=-0.8612003721446728, w1=-0.07645801848204528\n",
      "Gradient Descent(86/299): loss=100505.72182900619, w0=-0.8615676831529967, w1=-0.07581951140443005\n",
      "Gradient Descent(87/299): loss=100493.66932157845, w0=-0.8619301679812728, w1=-0.07516956878852077\n",
      "Gradient Descent(88/299): loss=100481.87513795933, w0=-0.8622879375280422, w1=-0.07450870859763904\n",
      "Gradient Descent(89/299): loss=100470.33197175362, w0=-0.8626410980141819, w1=-0.07383743274000976\n",
      "Gradient Descent(90/299): loss=100459.0327889512, w0=-0.8629897512595086, w1=-0.07315622758994389\n",
      "Gradient Descent(91/299): loss=100447.97081521947, w0=-0.8633339949437701, w1=-0.0724655644876759\n",
      "Gradient Descent(92/299): loss=100437.13952390288, w0=-0.863673922852407, w1=-0.07176590021875139\n",
      "Gradient Descent(93/299): loss=100426.53262468337, w0=-0.8640096251075468, w1=-0.07105767747386184\n",
      "Gradient Descent(94/299): loss=100416.1440528568, w0=-0.8643411883847484, w1=-0.07034132529001576\n",
      "Gradient Descent(95/299): loss=100405.9679591882, w0=-0.8646686961160631, w1=-0.06961725947392045\n",
      "Gradient Descent(96/299): loss=100395.998700307, w0=-0.864992228680013, w1=-0.06888588300843043\n",
      "Gradient Descent(97/299): loss=100386.23082961008, w0=-0.8653118635791082, w1=-0.06814758644289409\n",
      "Gradient Descent(98/299): loss=100376.65908864062, w0=-0.8656276756055394, w1=-0.0674027482682044\n",
      "Gradient Descent(99/299): loss=100367.2783989139, w0=-0.8659397369956859, w1=-0.06665173527733084\n",
      "Gradient Descent(100/299): loss=100358.08385416307, w0=-0.8662481175740747, w1=-0.06589490291207965\n",
      "Gradient Descent(101/299): loss=100349.07071298068, w0=-0.8665528848874198, w1=-0.06513259559679865\n",
      "Gradient Descent(102/299): loss=100340.23439183159, w0=-0.8668541043293551, w1=-0.06436514705971133\n",
      "Gradient Descent(103/299): loss=100331.57045841671, w0=-0.8671518392564563, w1=-0.06359288064253327\n",
      "Gradient Descent(104/299): loss=100323.07462536704, w0=-0.8674461510961278, w1=-0.06281610959899278\n",
      "Gradient Descent(105/299): loss=100314.74274424899, w0=-0.8677370994469035, w1=-0.06203513738284574\n",
      "Gradient Descent(106/299): loss=100306.57079986425, w0=-0.8680247421716911, w1=-0.061250257925944906\n",
      "Gradient Descent(107/299): loss=100298.55490482686, w0=-0.8683091354844577, w1=-0.06046175590689306\n",
      "Gradient Descent(108/299): loss=100290.69129440312, w0=-0.8685903340308314, w1=-0.059669907010780524\n",
      "Gradient Descent(109/299): loss=100282.97632159884, w0=-0.8688683909630666, w1=-0.05887497818047897\n",
      "Gradient Descent(110/299): loss=100275.40645248182, w0=-0.8691433580097916, w1=-0.05807722785993569\n",
      "Gradient Descent(111/299): loss=100267.97826172605, w0=-0.8694152855409352, w1=-0.057276906229885544\n",
      "Gradient Descent(112/299): loss=100260.6884283654, w0=-0.8696842226281989, w1=-0.05647425543637254\n",
      "Gradient Descent(113/299): loss=100253.53373174724, w0=-0.8699502171014204, w1=-0.05566950981244707\n",
      "Gradient Descent(114/299): loss=100246.51104767367, w0=-0.8702133156011482, w1=-0.05486289609338163\n",
      "Gradient Descent(115/299): loss=100239.61734472177, w0=-0.8704735636277251, w1=-0.05405463362572458\n",
      "Gradient Descent(116/299): loss=100232.8496807328, w0=-0.8707310055871565, w1=-0.053244934570489094\n",
      "Gradient Descent(117/299): loss=100226.20519946222, w0=-0.8709856848340202, w1=-0.05243400410075366\n",
      "Gradient Descent(118/299): loss=100219.68112738151, w0=-0.8712376437116517, w1=-0.05162204059393024\n",
      "Gradient Descent(119/299): loss=100213.27477062517, w0=-0.8714869235898265, w1=-0.050809235818936864\n",
      "Gradient Descent(120/299): loss=100206.98351207394, w0=-0.8717335649001364, w1=-0.049995775118493534\n",
      "Gradient Descent(121/299): loss=100200.8048085687, w0=-0.8719776071692477, w1=-0.04918183758674273\n",
      "Gradient Descent(122/299): loss=100194.73618824761, w0=-0.8722190890502094, w1=-0.04836759624237982\n",
      "Gradient Descent(123/299): loss=100188.77524800085, w0=-0.8724580483519688, w1=-0.04755321819746353\n",
      "Gradient Descent(124/299): loss=100182.91965103638, w0=-0.8726945220672362, w1=-0.04673886482206179\n",
      "Gradient Descent(125/299): loss=100177.1671245519, w0=-0.8729285463988301, w1=-0.04592469190487579\n",
      "Gradient Descent(126/299): loss=100171.5154575074, w0=-0.8731601567846233, w1=-0.04511084980997205\n",
      "Gradient Descent(127/299): loss=100165.96249849277, w0=-0.8733893879211987, w1=-0.04429748362974135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(128/299): loss=100160.50615368714, w0=-0.873616273786315, w1=-0.04348473333419275\n",
      "Gradient Descent(129/299): loss=100155.14438490389, w0=-0.8738408476602739, w1=-0.042672733916681464\n",
      "Gradient Descent(130/299): loss=100149.875207718, w0=-0.8740631421462711, w1=-0.04186161553616063\n",
      "Gradient Descent(131/299): loss=100144.69668967181, w0=-0.8742831891898085, w1=-0.04105150365603944\n",
      "Gradient Descent(132/299): loss=100139.60694855446, w0=-0.874501020097235, w1=-0.040242519179723026\n",
      "Gradient Descent(133/299): loss=100134.60415075294, w0=-0.8747166655534789, w1=-0.03943477858290341\n",
      "Gradient Descent(134/299): loss=100129.68650966945, w0=-0.8749301556390312, w1=-0.038628394042665555\n",
      "Gradient Descent(135/299): loss=100124.85228420355, w0=-0.8751415198462279, w1=-0.03782347356346753\n",
      "Gradient Descent(136/299): loss=100120.09977729485, w0=-0.8753507870948823, w1=-0.03702012110005063\n",
      "Gradient Descent(137/299): loss=100115.42733452405, w0=-0.8755579857473084, w1=-0.036218436677330934\n",
      "Gradient Descent(138/299): loss=100110.83334276889, w0=-0.8757631436227737, w1=-0.03541851650732189\n",
      "Gradient Descent(139/299): loss=100106.31622891326, w0=-0.8759662880114198, w1=-0.03462045310313484\n",
      "Gradient Descent(140/299): loss=100101.87445860621, w0=-0.8761674456876796, w1=-0.03382433539010291\n",
      "Gradient Descent(141/299): loss=100097.50653506878, w0=-0.8763666429232226, w1=-0.03303024881407183\n",
      "Gradient Descent(142/299): loss=100093.21099794669, w0=-0.8765639054994562, w1=-0.03223827544690148\n",
      "Gradient Descent(143/299): loss=100088.9864222063, w0=-0.8767592587196036, w1=-0.03144849408921991\n",
      "Gradient Descent(144/299): loss=100084.83141707233, w0=-0.8769527274203853, w1=-0.030660980370472785\n",
      "Gradient Descent(145/299): loss=100080.74462500513, w0=-0.8771443359833212, w1=-0.02987580684630977\n",
      "Gradient Descent(146/299): loss=100076.72472071546, w0=-0.8773341083456735, w1=-0.029093043093350788\n",
      "Gradient Descent(147/299): loss=100072.77041021588, w0=-0.8775220680110479, w1=-0.02831275580137457\n",
      "Gradient Descent(148/299): loss=100068.88042990648, w0=-0.8777082380596679, w1=-0.027535008862972714\n",
      "Gradient Descent(149/299): loss=100065.0535456934, w0=-0.8778926411583376, w1=-0.026759863460713178\n",
      "Gradient Descent(150/299): loss=100061.28855213913, w0=-0.8780752995701063, w1=-0.025987378151857428\n",
      "Gradient Descent(151/299): loss=100057.58427164276, w0=-0.8782562351636473, w1=-0.025217608950676308\n",
      "Gradient Descent(152/299): loss=100053.93955364928, w0=-0.8784354694223626, w1=-0.02445060940841048\n",
      "Gradient Descent(153/299): loss=100050.3532738864, w0=-0.8786130234532249, w1=-0.023686430690921797\n",
      "Gradient Descent(154/299): loss=100046.82433362753, w0=-0.878788917995366, w1=-0.02292512165408245\n",
      "Gradient Descent(155/299): loss=100043.35165898078, w0=-0.8789631734284216, w1=-0.022166728916949853\n",
      "Gradient Descent(156/299): loss=100039.93420020127, w0=-0.8791358097806423, w1=-0.021411296932775003\n",
      "Gradient Descent(157/299): loss=100036.57093102767, w0=-0.8793068467367778, w1=-0.02065886805789317\n",
      "Gradient Descent(158/299): loss=100033.26084804002, w0=-0.8794763036457435, w1=-0.01990948261854574\n",
      "Gradient Descent(159/299): loss=100030.00297004, w0=-0.8796441995280756, w1=-0.01916317897568243\n",
      "Gradient Descent(160/299): loss=100026.79633745085, w0=-0.8798105530831842, w1=-0.018419993587793206\n",
      "Gradient Descent(161/299): loss=100023.64001173739, w0=-0.879975382696409, w1=-0.01767996107181946\n",
      "Gradient Descent(162/299): loss=100020.53307484492, w0=-0.8801387064458848, w1=-0.016943114262193838\n",
      "Gradient Descent(163/299): loss=100017.47462865623, w0=-0.8803005421092243, w1=-0.016209484268058048\n",
      "Gradient Descent(164/299): loss=100014.46379446608, w0=-0.8804609071700221, w1=-0.015479100528707794\n",
      "Gradient Descent(165/299): loss=100011.49971247267, w0=-0.8806198188241877, w1=-0.01475199086731356\n",
      "Gradient Descent(166/299): loss=100008.58154128476, w0=-0.8807772939861107, w1=-0.014028181542965902\n",
      "Gradient Descent(167/299): loss=100005.70845744482, w0=-0.8809333492946662, w1=-0.013307697301092908\n",
      "Gradient Descent(168/299): loss=100002.87965496688, w0=-0.881088001119063, w1=-0.01259056142229734\n",
      "Gradient Descent(169/299): loss=100000.09434488886, w0=-0.8812412655645419, w1=-0.011876795769660373\n",
      "Gradient Descent(170/299): loss=99997.3517548386, w0=-0.8813931584779253, w1=-0.01116642083455768\n",
      "Gradient Descent(171/299): loss=99994.65112861336, w0=-0.8815436954530278, w1=-0.01045945578103349\n",
      "Gradient Descent(172/299): loss=99991.99172577221, w0=-0.8816928918359267, w1=-0.009755918488776818\n",
      "Gradient Descent(173/299): loss=99989.3728212405, w0=-0.881840762730101, w1=-0.009055825594743779\n",
      "Gradient Descent(174/299): loss=99986.79370492665, w0=-0.8819873230014401, w1=-0.008359192533468453\n",
      "Gradient Descent(175/299): loss=99984.25368135044, w0=-0.8821325872831278, w1=-0.007666033576104232\n",
      "Gradient Descent(176/299): loss=99981.75206928191, w0=-0.8822765699804049, w1=-0.006976361868236342\n",
      "Gradient Descent(177/299): loss=99979.28820139145, w0=-0.8824192852752137, w1=-0.006290189466505513\n",
      "Gradient Descent(178/299): loss=99976.86142391019, w0=-0.8825607471307287, w1=-0.0056075273740813095\n",
      "Gradient Descent(179/299): loss=99974.47109629998, w0=-0.8827009692957762, w1=-0.004928385575023175\n",
      "Gradient Descent(180/299): loss=99972.11659093361, w0=-0.8828399653091464, w1=-0.004252773067565631\n",
      "Gradient Descent(181/299): loss=99969.79729278362, w0=-0.8829777485038015, w1=-0.003580697896363371\n",
      "Gradient Descent(182/299): loss=99967.51259912072, w0=-0.8831143320109823, w1=-0.0029121671837310254\n",
      "Gradient Descent(183/299): loss=99965.26191922056, w0=-0.883249728764217, w1=-0.002247187159910792\n",
      "Gradient Descent(184/299): loss=99963.04467407857, w0=-0.8833839515032338, w1=-0.001585763192400769\n",
      "Gradient Descent(185/299): loss=99960.86029613396, w0=-0.8835170127777814, w1=-0.0009278998143753811\n",
      "Gradient Descent(186/299): loss=99958.70822900017, w0=-0.8836489249513598, w1=-0.0002736007522281908\n",
      "Gradient Descent(187/299): loss=99956.58792720395, w0=-0.8837797002048632, w1=0.0003771310477331814\n",
      "Gradient Descent(188/299): loss=99954.49885593113, w0=-0.883909350540138, w1=0.0010242933934118093\n",
      "Gradient Descent(189/299): loss=99952.44049077986, w0=-0.8840378877834585, w1=0.001667884821838184\n",
      "Gradient Descent(190/299): loss=99950.41231752053, w0=-0.8841653235889223, w1=0.002307904574852574\n",
      "Gradient Descent(191/299): loss=99948.41383186221, w0=-0.8842916694417677, w1=0.0029443525755271606\n",
      "Gradient Descent(192/299): loss=99946.44453922569, w0=-0.8844169366616154, w1=0.0035772294053034942\n",
      "Gradient Descent(193/299): loss=99944.50395452254, w0=-0.8845411364056355, w1=0.004206536281821289\n",
      "Gradient Descent(194/299): loss=99942.59160194047, w0=-0.8846642796716444, w1=0.004832275037415648\n",
      "Gradient Descent(195/299): loss=99940.70701473398, w0=-0.8847863773011304, w1=0.005454448098260524\n",
      "Gradient Descent(196/299): loss=99938.84973502114, w0=-0.8849074399822118, w1=0.006073058464137162\n",
      "Gradient Descent(197/299): loss=99937.01931358574, w0=-0.8850274782525286, w1=0.006688109688806944\n",
      "Gradient Descent(198/299): loss=99935.21530968428, w0=-0.8851465025020704, w1=0.007299605860968829\n",
      "Gradient Descent(199/299): loss=99933.43729085865, w0=-0.8852645229759406, w1=0.007907551585782388\n",
      "Gradient Descent(200/299): loss=99931.68483275337, w0=-0.8853815497770601, w1=0.008511951966938039\n",
      "Gradient Descent(201/299): loss=99929.95751893785, w0=-0.8854975928688109, w1=0.009112812589256763\n",
      "Gradient Descent(202/299): loss=99928.25494073314, w0=-0.8856126620776222, w1=0.009710139501802464\n",
      "Gradient Descent(203/299): loss=99926.57669704313, w0=-0.8857267670954997, w1=0.010303939201490322\n",
      "Gradient Descent(204/299): loss=99924.9223941906, w0=-0.8858399174824988, w1=0.010894218617175662\n",
      "Gradient Descent(205/299): loss=99923.29164575665, w0=-0.8859521226691456, w1=0.011480985094207922\n",
      "Gradient Descent(206/299): loss=99921.68407242512, w0=-0.8860633919588041, w1=0.012064246379435161\n",
      "Gradient Descent(207/299): loss=99920.0993018305, w0=-0.8861737345299929, w1=0.012644010606645068\n",
      "Gradient Descent(208/299): loss=99918.53696840964, w0=-0.8862831594386515, w1=0.013220286282428875\n",
      "Gradient Descent(209/299): loss=99916.99671325742, w0=-0.886391675620358, w1=0.013793082272455041\n",
      "Gradient Descent(210/299): loss=99915.47818398618, w0=-0.8864992918924995, w1=0.014362407788140292\n",
      "Gradient Descent(211/299): loss=99913.98103458827, w0=-0.8866060169563956, w1=0.014928272373705517\n",
      "Gradient Descent(212/299): loss=99912.5049253024, w0=-0.8867118593993771, w1=0.015490685893605353\n",
      "Gradient Descent(213/299): loss=99911.0495224829, w0=-0.8868168276968199, w1=0.016049658520319667\n",
      "Gradient Descent(214/299): loss=99909.61449847283, w0=-0.8869209302141361, w1=0.0166052007224964\n",
      "Gradient Descent(215/299): loss=99908.1995314796, w0=-0.8870241752087226, w1=0.017157323253435133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(216/299): loss=99906.80430545378, w0=-0.8871265708318686, w1=0.01770603713990133\n",
      "Gradient Descent(217/299): loss=99905.42850997129, w0=-0.8872281251306233, w1=0.018251353671261447\n",
      "Gradient Descent(218/299): loss=99904.0718401178, w0=-0.8873288460496233, w1=0.01879328438892949\n",
      "Gradient Descent(219/299): loss=99902.73399637634, w0=-0.8874287414328821, w1=0.01933184107611603\n",
      "Gradient Descent(220/299): loss=99901.41468451753, w0=-0.8875278190255426, w1=0.01986703574787068\n",
      "Gradient Descent(221/299): loss=99900.1136154925, w0=-0.8876260864755919, w1=0.020398880641409822\n",
      "Gradient Descent(222/299): loss=99898.83050532802, w0=-0.887723551335541, w1=0.020927388206721166\n",
      "Gradient Descent(223/299): loss=99897.56507502457, w0=-0.887820221064069, w1=0.021452571097437353\n",
      "Gradient Descent(224/299): loss=99896.31705045662, w0=-0.8879161030276331, w1=0.02197444216197085\n",
      "Gradient Descent(225/299): loss=99895.08616227511, w0=-0.888011204502046, w1=0.022493014434902852\n",
      "Gradient Descent(226/299): loss=99893.87214581243, w0=-0.8881055326740189, w1=0.023008301128618923\n",
      "Gradient Descent(227/299): loss=99892.67474098955, w0=-0.8881990946426743, w1=0.02352031562518446\n",
      "Gradient Descent(228/299): loss=99891.4936922253, w0=-0.8882918974210263, w1=0.02402907146845343\n",
      "Gradient Descent(229/299): loss=99890.32874834765, w0=-0.8883839479374308, w1=0.02453458235640365\n",
      "Gradient Descent(230/299): loss=99889.17966250716, w0=-0.8884752530370057, w1=0.025036862133692415\n",
      "Gradient Descent(231/299): loss=99888.04619209247, w0=-0.8885658194830223, w1=0.02553592478442663\n",
      "Gradient Descent(232/299): loss=99886.9280986474, w0=-0.8886556539582672, w1=0.026031784425141053\n",
      "Gradient Descent(233/299): loss=99885.82514779028, w0=-0.8887447630663775, w1=0.026524455297979446\n",
      "Gradient Descent(234/299): loss=99884.73710913498, w0=-0.8888331533331475, w1=0.027013951764072747\n",
      "Gradient Descent(235/299): loss=99883.66375621366, w0=-0.8889208312078095, w1=0.027500288297109116\n",
      "Gradient Descent(236/299): loss=99882.60486640122, w0=-0.8890078030642881, w1=0.027983479477090397\n",
      "Gradient Descent(237/299): loss=99881.56022084173, w0=-0.889094075202429, w1=0.028463539984270265\n",
      "Gradient Descent(238/299): loss=99880.5296043761, w0=-0.8891796538492029, w1=0.028940484593268864\n",
      "Gradient Descent(239/299): loss=99879.51280547163, w0=-0.8892645451598853, w1=0.02941432816735939\n",
      "Gradient Descent(240/299): loss=99878.5096161529, w0=-0.8893487552192113, w1=0.02988508565292189\n",
      "Gradient Descent(241/299): loss=99877.51983193448, w0=-0.8894322900425086, w1=0.030352772074059727\n",
      "Gradient Descent(242/299): loss=99876.54325175472, w0=-0.8895151555768062, w1=0.030817402527374586\n",
      "Gradient Descent(243/299): loss=99875.57967791139, w0=-0.8895973577019216, w1=0.03127899217689565\n",
      "Gradient Descent(244/299): loss=99874.6289159983, w0=-0.8896789022315263, w1=0.031737556249158756\n",
      "Gradient Descent(245/299): loss=99873.69077484381, w0=-0.889759794914189, w1=0.03219311002843165\n",
      "Gradient Descent(246/299): loss=99872.76506645013, w0=-0.8898400414343985, w1=0.03264566885208155\n",
      "Gradient Descent(247/299): loss=99871.8516059342, w0=-0.8899196474135666, w1=0.033095248106081006\n",
      "Gradient Descent(248/299): loss=99870.95021146983, w0=-0.8899986184110097, w1=0.033541863220648595\n",
      "Gradient Descent(249/299): loss=99870.06070423106, w0=-0.8900769599249124, w1=0.03398552966602079\n",
      "Gradient Descent(250/299): loss=99869.18290833662, w0=-0.8901546773932708, w1=0.03442626294835156\n",
      "Gradient Descent(251/299): loss=99868.31665079584, w0=-0.8902317761948173, w1=0.03486407860573639\n",
      "Gradient Descent(252/299): loss=99867.46176145511, w0=-0.8903082616499275, w1=0.035298992204357275\n",
      "Gradient Descent(253/299): loss=99866.61807294638, w0=-0.8903841390215089, w1=0.03573101933474571\n",
      "Gradient Descent(254/299): loss=99865.78542063576, w0=-0.8904594135158722, w1=0.03616017560816041\n",
      "Gradient Descent(255/299): loss=99864.96364257383, w0=-0.8905340902835853, w1=0.036586476653076766\n",
      "Gradient Descent(256/299): loss=99864.152579447, w0=-0.8906081744203106, w1=0.037009938111785184\n",
      "Gradient Descent(257/299): loss=99863.35207452915, w0=-0.8906816709676261, w1=0.03743057563709534\n",
      "Gradient Descent(258/299): loss=99862.56197363551, w0=-0.89075458491383, w1=0.037848404889143594\n",
      "Gradient Descent(259/299): loss=99861.782125076, w0=-0.8908269211947304, w1=0.038263441532300914\n",
      "Gradient Descent(260/299): loss=99861.01237961097, w0=-0.8908986846944189, w1=0.03867570123217853\n",
      "Gradient Descent(261/299): loss=99860.25259040676, w0=-0.8909698802460294, w1=0.03908519965272888\n",
      "Gradient Descent(262/299): loss=99859.50261299268, w0=-0.8910405126324824, w1=0.039491952453439255\n",
      "Gradient Descent(263/299): loss=99858.76230521887, w0=-0.8911105865872151, w1=0.0398959752866158\n",
      "Gradient Descent(264/299): loss=99858.03152721471, w0=-0.8911801067948966, w1=0.04029728379475527\n",
      "Gradient Descent(265/299): loss=99857.31014134848, w0=-0.8912490778921304, w1=0.040695893608002576\n",
      "Gradient Descent(266/299): loss=99856.59801218737, w0=-0.891317504468143, w1=0.04109182034169144\n",
      "Gradient Descent(267/299): loss=99855.89500645878, w0=-0.8913853910654591, w1=0.04148507959396642\n",
      "Gradient Descent(268/299): loss=99855.20099301185, w0=-0.8914527421805644, w1=0.04187568694348373\n",
      "Gradient Descent(269/299): loss=99854.51584278037, w0=-0.8915195622645554, w1=0.04226365794718906\n",
      "Gradient Descent(270/299): loss=99853.83942874591, w0=-0.8915858557237776, w1=0.042649008138170255\n",
      "Gradient Descent(271/299): loss=99853.17162590187, w0=-0.8916516269204503, w1=0.04303175302358284\n",
      "Gradient Descent(272/299): loss=99852.51231121832, w0=-0.8917168801732812, w1=0.04341190808264652\n",
      "Gradient Descent(273/299): loss=99851.86136360746, w0=-0.8917816197580685, w1=0.04378948876471071\n",
      "Gradient Descent(274/299): loss=99851.2186638896, w0=-0.8918458499082919, w1=0.0441645104873874\n",
      "Gradient Descent(275/299): loss=99850.58409476021, w0=-0.8919095748156927, w1=0.04453698863474932\n",
      "Gradient Descent(276/299): loss=99849.95754075711, w0=-0.8919727986308431, w1=0.044906938555591865\n",
      "Gradient Descent(277/299): loss=99849.33888822875, w0=-0.8920355254637047, w1=0.04527437556175702\n",
      "Gradient Descent(278/299): loss=99848.72802530273, w0=-0.892097759384177, w1=0.04563931492651764\n",
      "Gradient Descent(279/299): loss=99848.12484185518, w0=-0.8921595044226358, w1=0.046001771883020355\n",
      "Gradient Descent(280/299): loss=99847.52922948076, w0=-0.8922207645704611, w1=0.04636176162278574\n",
      "Gradient Descent(281/299): loss=99846.94108146279, w0=-0.892281543780556, w1=0.046719299294264036\n",
      "Gradient Descent(282/299): loss=99846.36029274468, w0=-0.8923418459678562, w1=0.04707440000144496\n",
      "Gradient Descent(283/299): loss=99845.78675990136, w0=-0.8924016750098294, w1=0.04742707880252018\n",
      "Gradient Descent(284/299): loss=99845.2203811112, w0=-0.8924610347469667, w1=0.0477773507085971\n",
      "Gradient Descent(285/299): loss=99844.66105612909, w0=-0.8925199289832643, w1=0.04812523068246235\n",
      "Gradient Descent(286/299): loss=99844.1086862592, w0=-0.8925783614866967, w1=0.048470733637393855\n",
      "Gradient Descent(287/299): loss=99843.563174329, w0=-0.8926363359896818, w1=0.04881387443602006\n",
      "Gradient Descent(288/299): loss=99843.02442466315, w0=-0.8926938561895377, w1=0.04915466788922502\n",
      "Gradient Descent(289/299): loss=99842.49234305843, w0=-0.8927509257489301, w1=0.04949312875509809\n",
      "Gradient Descent(290/299): loss=99841.96683675869, w0=-0.8928075482963139, w1=0.04982927173792717\n",
      "Gradient Descent(291/299): loss=99841.44781443046, w0=-0.8928637274263651, w1=0.05016311148723398\n",
      "Gradient Descent(292/299): loss=99840.93518613922, w0=-0.892919466700406, w1=0.05049466259685044\n",
      "Gradient Descent(293/299): loss=99840.42886332565, w0=-0.8929747696468228, w1=0.05082393960403508\n",
      "Gradient Descent(294/299): loss=99839.92875878274, w0=-0.8930296397614756, w1=0.05115095698862817\n",
      "Gradient Descent(295/299): loss=99839.43478663312, w0=-0.8930840805081023, w1=0.05147572917224465\n",
      "Gradient Descent(296/299): loss=99838.94686230677, w0=-0.8931380953187136, w1=0.05179827051750359\n",
      "Gradient Descent(297/299): loss=99838.46490251936, w0=-0.8931916875939829, w1=0.052118595327293585\n",
      "Gradient Descent(298/299): loss=99837.98882525077, w0=-0.8932448607036292, w1=0.05243671784407266\n",
      "Gradient Descent(299/299): loss=99837.51854972406, w0=-0.893297617986792, w1=0.05275265224920184\n",
      "Gradient Descent(0/299): loss=138629.43611198495, w0=-0.15722000000000003, w1=0.004575331255768596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/299): loss=122179.0038526293, w0=-0.27506976415202045, w1=0.007127140761025218\n",
      "Gradient Descent(2/299): loss=115428.22023511097, w0=-0.36818797887684074, w1=0.008751188085472266\n",
      "Gradient Descent(3/299): loss=111867.84301425586, w0=-0.44229897103711446, w1=0.007757699878277273\n",
      "Gradient Descent(4/299): loss=109656.03032001274, w0=-0.5021548690424237, w1=0.005329811496933387\n",
      "Gradient Descent(5/299): loss=108173.8516527329, w0=-0.5510230115418528, w1=0.0020234722365839886\n",
      "Gradient Descent(6/299): loss=107126.5608366137, w0=-0.5912771046279626, w1=-0.001758589311772146\n",
      "Gradient Descent(7/299): loss=106354.5916684398, w0=-0.624694130728103, w1=-0.005778261989143393\n",
      "Gradient Descent(8/299): loss=105764.71038013852, w0=-0.6526269061203834, w1=-0.00988904413161346\n",
      "Gradient Descent(9/299): loss=105299.535179469, w0=-0.6761221331286094, w1=-0.014001515418981463\n",
      "Gradient Descent(10/299): loss=104922.34098537927, w0=-0.6959993819133106, w1=-0.018060704337798307\n",
      "Gradient Descent(11/299): loss=104608.87868753911, w0=-0.7129069958057797, w1=-0.022032791238980807\n",
      "Gradient Descent(12/299): loss=104342.71368168615, w0=-0.7273623840665886, w1=-0.02589709676666169\n",
      "Gradient Descent(13/299): loss=104112.45189153237, w0=-0.7397816710588838, w1=-0.029641234516366913\n",
      "Gradient Descent(14/299): loss=103910.03117672353, w0=-0.7505018487094727, w1=-0.03325813931391507\n",
      "Gradient Descent(15/299): loss=103729.63889840661, w0=-0.7597975429488821, w1=-0.03674421849861316\n",
      "Gradient Descent(16/299): loss=103567.00973283469, w0=-0.7678938434886748, w1=-0.04009818186353672\n",
      "Gradient Descent(17/299): loss=103418.96075615418, w0=-0.7749762155888646, w1=-0.043320286501944055\n",
      "Gradient Descent(18/299): loss=103283.07804145382, w0=-0.7811982229252665, w1=-0.04641183882301002\n",
      "Gradient Descent(19/299): loss=103157.50196336722, w0=-0.786687591429956, w1=-0.04937485868438738\n",
      "Gradient Descent(20/299): loss=103040.77795693126, w0=-0.7915510041691407, w1=-0.05221184788825539\n",
      "Gradient Descent(21/299): loss=102931.75137385972, w0=-0.7958779176076818, w1=-0.05492562759336459\n",
      "Gradient Descent(22/299): loss=102829.4924780296, w0=-0.7997436175066999, w1=-0.05751922257485193\n",
      "Gradient Descent(23/299): loss=102733.24231255692, w0=-0.8032116799436466, w1=-0.059995778293852865\n",
      "Gradient Descent(24/299): loss=102642.373196003, w0=-0.806335963938052, w1=-0.062358501555303134\n",
      "Gradient Descent(25/299): loss=102556.35958698511, w0=-0.8091622330557156, w1=-0.06461061842762558\n",
      "Gradient Descent(26/299): loss=102474.75637355442, w0=-0.8117294814538639, w1=-0.066755344865333\n",
      "Gradient Descent(27/299): loss=102397.18253099074, w0=-0.814071023211905, w1=-0.06879586660174852\n",
      "Gradient Descent(28/299): loss=102323.30869701327, w0=-0.8162153911015577, w1=-0.07073532565489023\n",
      "Gradient Descent(29/299): loss=102252.8476312139, w0=-0.8181870811983923, w1=-0.07257681137736424\n",
      "Gradient Descent(30/299): loss=102185.54681685686, w0=-0.8200071722011133, w1=-0.07432335446092077\n",
      "Gradient Descent(31/299): loss=102121.18266809185, w0=-0.821693842469672, w1=-0.07597792271057126\n",
      "Gradient Descent(32/299): loss=102059.55595071311, w0=-0.8232628032173008, w1=-0.07754341774198183\n",
      "Gradient Descent(33/299): loss=102000.488127783, w0=-0.8247276626908406, w1=-0.07902267203370755\n",
      "Gradient Descent(34/299): loss=101943.8184149471, w0=-0.826100233315666, w1=-0.08041844599034169\n",
      "Gradient Descent(35/299): loss=101889.40138252024, w0=-0.827390791486563, w1=-0.08173342485732374\n",
      "Gradient Descent(36/299): loss=101837.10497842389, w0=-0.8286082978158569, w1=-0.08297021549071885\n",
      "Gradient Descent(37/299): loss=101786.80887229773, w0=-0.8297605841020818, w1=-0.08413134314268625\n",
      "Gradient Descent(38/299): loss=101738.40304041491, w0=-0.8308545119892954, w1=-0.0852192485836338\n",
      "Gradient Descent(39/299): loss=101691.78652705766, w0=-0.8318961072216439, w1=-0.08623628603409202\n",
      "Gradient Descent(40/299): loss=101646.86633429455, w0=-0.83289067257606, w1=-0.08718472248446917\n",
      "Gradient Descent(41/299): loss=101603.55641125164, w0=-0.8338428820268403, w1=-0.08806673897523046\n",
      "Gradient Descent(42/299): loss=101561.77673594959, w0=-0.8347568585039854, w1=-0.08888443422875345\n",
      "Gradient Descent(43/299): loss=101521.45250338287, w0=-0.8356362377312183, w1=-0.08963983064805098\n",
      "Gradient Descent(44/299): loss=101482.5134450672, w0=-0.8364842209281343, w1=-0.09033488219966856\n",
      "Gradient Descent(45/299): loss=101444.89330113816, w0=-0.8373036193832374, w1=-0.09097148324412722\n",
      "Gradient Descent(46/299): loss=101408.52944665236, w0=-0.8380968937952209, w1=-0.09155147715382027\n",
      "Gradient Descent(47/299): loss=101373.36264841292, w0=-0.8388661907256227, w1=-0.09207666366275973\n",
      "Gradient Descent(48/299): loss=101339.33691030384, w0=-0.8396133776141586, w1=-0.09254880427071357\n",
      "Gradient Descent(49/299): loss=101306.39936156053, w0=-0.8403400768458543, w1=-0.09296962550843903\n",
      "Gradient Descent(50/299): loss=101274.50015216938, w0=-0.8410476986007016, w1=-0.09334082028434659\n",
      "Gradient Descent(51/299): loss=101243.59233526117, w0=-0.8417374718120466, w1=-0.09366404777625349\n",
      "Gradient Descent(52/299): loss=101213.63173055799, w0=-0.8424104725025315, w1=-0.09394093239921927\n",
      "Gradient Descent(53/299): loss=101184.57677195988, w0=-0.8430676489457541, w1=-0.0941730623244855\n",
      "Gradient Descent(54/299): loss=101156.38834614016, w0=-0.8437098433823751, w1=-0.09436198791048338\n",
      "Gradient Descent(55/299): loss=101129.02962924662, w0=-0.8443378102952942, w1=-0.09450922028440144\n",
      "Gradient Descent(56/299): loss=101102.46592734147, w0=-0.8449522314616634, w1=-0.09461623020886122\n",
      "Gradient Descent(57/299): loss=101076.66452432176, w0=-0.8455537281326738, w1=-0.09468444729177898\n",
      "Gradient Descent(58/299): loss=101051.59453941314, w0=-0.8461428707529601, w1=-0.09471525954712678\n",
      "Gradient Descent(59/299): loss=101027.22679509863, w0=-0.8467201866383044, w1=-0.0947100132845221\n",
      "Gradient Descent(60/299): loss=101003.53369555355, w0=-0.8472861660025091, w1=-0.09467001329025934\n",
      "Gradient Descent(61/299): loss=100980.48911517573, w0=-0.8478412666779823, w1=-0.09459652325642086\n",
      "Gradient Descent(62/299): loss=100958.06829656256, w0=-0.8483859178213339, w1=-0.0944907664143109\n",
      "Gradient Descent(63/299): loss=100936.24775718153, w0=-0.8489205228425051, w1=-0.09435392633112537\n",
      "Gradient Descent(64/299): loss=100915.00520395987, w0=-0.8494454617477858, w1=-0.09418714783291561\n",
      "Gradient Descent(65/299): loss=100894.31945505433, w0=-0.8499610930453864, w1=-0.09399153802158358\n",
      "Gradient Descent(66/299): loss=100874.17036810695, w0=-0.8504677553275043, w1=-0.09376816735832644\n",
      "Gradient Descent(67/299): loss=100854.5387743565, w0=-0.8509657686147043, w1=-0.09351807079035088\n",
      "Gradient Descent(68/299): loss=100835.4064180359, w0=-0.8514554355261834, w1=-0.0932422489016697\n",
      "Gradient Descent(69/299): loss=100816.75590054286, w0=-0.8519370423222262, w1=-0.09294166907232805\n",
      "Gradient Descent(70/299): loss=100798.57062892703, w0=-0.8524108598519882, w1=-0.09261726663348328\n",
      "Gradient Descent(71/299): loss=100780.8347682853, w0=-0.8528771444298673, w1=-0.09226994600840313\n",
      "Gradient Descent(72/299): loss=100763.53319770069, w0=-0.8533361386564337, w1=-0.09190058183168832\n",
      "Gradient Descent(73/299): loss=100746.65146939925, w0=-0.853788072194601, w1=-0.09151002004090765\n",
      "Gradient Descent(74/299): loss=100730.17577083423, w0=-0.8542331625079671, w1=-0.09109907893639711\n",
      "Gradient Descent(75/299): loss=100714.09288943824, w0=-0.8546716155656411, w1=-0.09066855020626158\n",
      "Gradient Descent(76/299): loss=100698.39017981004, w0=-0.8551036265161172, w1=-0.090219199914664\n",
      "Gradient Descent(77/299): loss=100683.05553312715, w0=-0.8555293803316185, w1=-0.08975176945233039\n",
      "Gradient Descent(78/299): loss=100668.07734859717, w0=-0.8559490524236416, w1=-0.08926697644886789\n",
      "Gradient Descent(79/299): loss=100653.44450678022, w0=-0.8563628092300504, w1=-0.08876551564701841\n",
      "Gradient Descent(80/299): loss=100639.14634462923, w0=-0.8567708087739029, w1=-0.08824805973937479\n",
      "Gradient Descent(81/299): loss=100625.17263211285, w0=-0.8571732011941623, w1=-0.08771526016839183\n",
      "Gradient Descent(82/299): loss=100611.51355029599, w0=-0.8575701292485034, w1=-0.08716774789074953\n",
      "Gradient Descent(83/299): loss=100598.15967076783, w0=-0.8579617287885317, w1=-0.08660613410728478\n",
      "Gradient Descent(84/299): loss=100585.10193631388, w0=-0.8583481292078617, w1=-0.08603101095981512\n",
      "Gradient Descent(85/299): loss=100572.33164274243, w0=-0.8587294538636341, w1=-0.08544295219624345\n",
      "Gradient Descent(86/299): loss=100559.84042177936, w0=-0.8591058204721793, w1=-0.08484251380536512\n",
      "Gradient Descent(87/299): loss=100547.62022495727, w0=-0.8594773414796482, w1=-0.08423023462280636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(88/299): loss=100535.66330842688, w0=-0.8598441244085244, w1=-0.08360663690951116\n",
      "Gradient Descent(89/299): loss=100523.96221862955, w0=-0.8602062721810115, w1=-0.08297222690416642\n",
      "Gradient Descent(90/299): loss=100512.50977877068, w0=-0.860563883420343, w1=-0.08232749535091878\n",
      "Gradient Descent(91/299): loss=100501.29907604157, w0=-0.8609170527311026, w1=-0.08167291800369066\n",
      "Gradient Descent(92/299): loss=100490.32344954029, w0=-0.8612658709596647, w1=-0.0810089561083535\n",
      "Gradient Descent(93/299): loss=100479.5764788463, w0=-0.8616104254358703, w1=-0.08033605686396231\n",
      "Gradient Descent(94/299): loss=100469.05197320698, w0=-0.8619508001970496, w1=-0.07965465386420072\n",
      "Gradient Descent(95/299): loss=100458.74396129833, w0=-0.8622870761954846, w1=-0.07896516752012961\n",
      "Gradient Descent(96/299): loss=100448.64668152298, w0=-0.8626193314903788, w1=-0.07826800546527694\n",
      "Gradient Descent(97/299): loss=100438.75457281418, w0=-0.8629476414253706, w1=-0.0775635629440518\n",
      "Gradient Descent(98/299): loss=100429.0622659135, w0=-0.8632720787925855, w1=-0.07685222318441198\n",
      "Gradient Descent(99/299): loss=100419.56457509549, w0=-0.8635927139841821, w1=-0.07613435775566367\n",
      "Gradient Descent(100/299): loss=100410.25649031205, w0=-0.8639096151323012, w1=-0.07541032691222127\n",
      "Gradient Descent(101/299): loss=100401.13316973239, w0=-0.8642228482382782, w1=-0.07468047992410873\n",
      "Gradient Descent(102/299): loss=100392.18993265496, w0=-0.8645324772919337, w1=-0.07394515539493808\n",
      "Gradient Descent(103/299): loss=100383.42225277201, w0=-0.8648385643817077, w1=-0.07320468156805798\n",
      "Gradient Descent(104/299): loss=100374.8257517644, w0=-0.8651411697963541, w1=-0.07245937662152403\n",
      "Gradient Descent(105/299): loss=100366.39619321043, w0=-0.8654403521188682, w1=-0.0717095489525041\n",
      "Gradient Descent(106/299): loss=100358.1294767893, w0=-0.8657361683132704, w1=-0.07095549745169513\n",
      "Gradient Descent(107/299): loss=100350.0216327649, w0=-0.8660286738048302, w1=-0.07019751176829322\n",
      "Gradient Descent(108/299): loss=100342.06881673317, w0=-0.8663179225542689, w1=-0.06943587256602589\n",
      "Gradient Descent(109/299): loss=100334.26730461947, w0=-0.8666039671264407, w1=-0.0686708517707246\n",
      "Gradient Descent(110/299): loss=100326.61348791268, w0=-0.8668868587539553, w1=-0.06790271280988606\n",
      "Gradient Descent(111/299): loss=100319.10386912318, w0=-0.8671666473961668, w1=-0.06713171084464332\n",
      "Gradient Descent(112/299): loss=100311.73505745281, w0=-0.867443381793922, w1=-0.0663580929945409\n",
      "Gradient Descent(113/299): loss=100304.50376466627, w0=-0.8677171095204301, w1=-0.06558209855548382\n",
      "Gradient Descent(114/299): loss=100297.4068011521, w0=-0.8679878770285836, w1=-0.0648039592112065\n",
      "Gradient Descent(115/299): loss=100290.44107216605, w0=-0.8682557296950357, w1=-0.06402389923858504\n",
      "Gradient Descent(116/299): loss=100283.60357424413, w0=-0.8685207118613134, w1=-0.06324213570709511\n",
      "Gradient Descent(117/299): loss=100276.89139177933, w0=-0.86878286687222, w1=-0.06245887867269791\n",
      "Gradient Descent(118/299): loss=100270.30169375255, w0=-0.8690422371117613, w1=-0.06167433136641676\n",
      "Gradient Descent(119/299): loss=100263.83173061015, w0=-0.8692988640368088, w1=-0.06088869037784934\n",
      "Gradient Descent(120/299): loss=100257.47883128031, w0=-0.8695527882086928, w1=-0.06010214583384301\n",
      "Gradient Descent(121/299): loss=100251.2404003223, w0=-0.8698040493229041, w1=-0.0593148815725443\n",
      "Gradient Descent(122/299): loss=100245.11391520101, w0=-0.8700526862370658, w1=-0.05852707531301831\n",
      "Gradient Descent(123/299): loss=100239.09692368086, w0=-0.8702987369973204, w1=-0.057738898820618854\n",
      "Gradient Descent(124/299): loss=100233.18704133347, w0=-0.8705422388632686, w1=-0.05695051806827652\n",
      "Gradient Descent(125/299): loss=100227.38194915296, w0=-0.8707832283315785, w1=-0.056162093393858796\n",
      "Gradient Descent(126/299): loss=100221.6793912742, w0=-0.8710217411583777, w1=-0.05537377965374402\n",
      "Gradient Descent(127/299): loss=100216.0771727883, w0=-0.8712578123805277, w1=-0.05458572637273959\n",
      "Gradient Descent(128/299): loss=100210.57315765144, w0=-0.8714914763358705, w1=-0.05379807789046397\n",
      "Gradient Descent(129/299): loss=100205.16526668161, w0=-0.871722766682532, w1=-0.053010973504302346\n",
      "Gradient Descent(130/299): loss=100199.85147563936, w0=-0.8719517164173537, w1=-0.05222454760903619\n",
      "Gradient Descent(131/299): loss=100194.62981338913, w0=-0.8721783578935229, w1=-0.05143892983323882\n",
      "Gradient Descent(132/299): loss=100189.49836013617, w0=-0.8724027228374616, w1=-0.05065424517252108\n",
      "Gradient Descent(133/299): loss=100184.45524573617, w0=-0.872624842365028, w1=-0.04987061411970422\n",
      "Gradient Descent(134/299): loss=100179.49864807396, w0=-0.8728447469970833, w1=-0.0490881527919904\n",
      "Gradient Descent(135/299): loss=100174.62679150782, w0=-0.8730624666744652, w1=-0.04830697305519589\n",
      "Gradient Descent(136/299): loss=100169.83794537678, w0=-0.8732780307724133, w1=-0.04752718264510637\n",
      "Gradient Descent(137/299): loss=100165.13042256705, w0=-0.8734914681144788, w1=-0.04674888528600956\n",
      "Gradient Descent(138/299): loss=100160.50257813564, w0=-0.8737028069859545, w1=-0.045972180806456105\n",
      "Gradient Descent(139/299): loss=100155.95280798821, w0=-0.8739120751468549, w1=-0.0451971652522965\n",
      "Gradient Descent(140/299): loss=100151.47954760859, w0=-0.8741192998444718, w1=-0.044423930997038535\n",
      "Gradient Descent(141/299): loss=100147.08127083706, w0=-0.8743245078255318, w1=-0.0436525668495675\n",
      "Gradient Descent(142/299): loss=100142.75648869635, w0=-0.8745277253479771, w1=-0.042883158159269255\n",
      "Gradient Descent(143/299): loss=100138.50374826175, w0=-0.8747289781923888, w1=-0.042115786918594675\n",
      "Gradient Descent(144/299): loss=100134.32163157451, w0=-0.874928291673073, w1=-0.041350531863102696\n",
      "Gradient Descent(145/299): loss=100130.20875459582, w0=-0.8751256906488233, w1=-0.04058746856901812\n",
      "Gradient Descent(146/299): loss=100126.16376619934, w0=-0.8753211995333776, w1=-0.039826669548340035\n",
      "Gradient Descent(147/299): loss=100122.18534720183, w0=-0.8755148423055807, w1=-0.03906820434153584\n",
      "Gradient Descent(148/299): loss=100118.27220942834, w0=-0.8757066425192659, w1=-0.038312139607856156\n",
      "Gradient Descent(149/299): loss=100114.42309481191, w0=-0.8758966233128671, w1=-0.03755853921330569\n",
      "Gradient Descent(150/299): loss=100110.6367745256, w0=-0.8760848074187721, w1=-0.03680746431630535\n",
      "Gradient Descent(151/299): loss=100106.91204814527, w0=-0.8762712171724258, w1=-0.036058973451081375\n",
      "Gradient Descent(152/299): loss=100103.24774284248, w0=-0.876455874521193, w1=-0.03531312260881746\n",
      "Gradient Descent(153/299): loss=100099.64271260527, w0=-0.8766388010329889, w1=-0.03456996531660654\n",
      "Gradient Descent(154/299): loss=100096.09583748638, w0=-0.876820017904685, w1=-0.03382955271423945\n",
      "Gradient Descent(155/299): loss=100092.6060228775, w0=-0.8769995459702974, w1=-0.03309193362886818\n",
      "Gradient Descent(156/299): loss=100089.17219880807, w0=-0.8771774057089643, w1=-0.03235715464758208\n",
      "Gradient Descent(157/299): loss=100085.79331926838, w0=-0.8773536172527192, w1=-0.031625260187936134\n",
      "Gradient Descent(158/299): loss=100082.46836155507, w0=-0.877528200394066, w1=-0.030896292566470623\n",
      "Gradient Descent(159/299): loss=100079.19632563897, w0=-0.8777011745933612, w1=-0.030170292065262522\n",
      "Gradient Descent(160/299): loss=100075.9762335535, w0=-0.8778725589860092, w1=-0.02944729699654906\n",
      "Gradient Descent(161/299): loss=100072.80712880372, w0=-0.8780423723894759, w1=-0.028727343765464507\n",
      "Gradient Descent(162/299): loss=100069.6880757942, w0=-0.8782106333101247, w1=-0.028010466930931618\n",
      "Gradient Descent(163/299): loss=100066.61815927588, w0=-0.8783773599498814, w1=-0.027296699264749417\n",
      "Gradient Descent(164/299): loss=100063.59648381057, w0=-0.8785425702127315, w1=-0.026586071808919143\n",
      "Gradient Descent(165/299): loss=100060.62217325263, w0=-0.8787062817110541, w1=-0.02587861393125068\n",
      "Gradient Descent(166/299): loss=100057.69437024713, w0=-0.8788685117717986, w1=-0.025174353379291248\n",
      "Gradient Descent(167/299): loss=100054.81223574377, w0=-0.8790292774425058, w1=-0.024473316332618855\n",
      "Gradient Descent(168/299): loss=100051.97494852613, w0=-0.8791885954971803, w1=-0.02377552745354222\n",
      "Gradient Descent(169/299): loss=100049.18170475555, w0=-0.8793464824420163, w1=-0.023081009936249244\n",
      "Gradient Descent(170/299): loss=100046.43171752908, w0=-0.879502954520981, w1=-0.02238978555444552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(171/299): loss=100043.72421645094, w0=-0.879658027721261, w1=-0.02170187470752429\n",
      "Gradient Descent(172/299): loss=100041.05844721734, w0=-0.8798117177785728, w1=-0.021017296465308602\n",
      "Gradient Descent(173/299): loss=100038.43367121351, w0=-0.8799640401823436, w1=-0.020336068611406334\n",
      "Gradient Descent(174/299): loss=100035.84916512313, w0=-0.8801150101807643, w1=-0.019658207685217876\n",
      "Gradient Descent(175/299): loss=100033.30422054931, w0=-0.8802646427857187, w1=-0.018983729022635926\n",
      "Gradient Descent(176/299): loss=100030.79814364706, w0=-0.8804129527775926, w1=-0.01831264679547618\n",
      "Gradient Descent(177/299): loss=100028.33025476633, w0=-0.8805599547099655, w1=-0.017644974049676965\n",
      "Gradient Descent(178/299): loss=100025.8998881059, w0=-0.8807056629141886, w1=-0.016980722742305308\n",
      "Gradient Descent(179/299): loss=100023.5063913771, w0=-0.8808500915038521, w1=-0.016319903777406077\n",
      "Gradient Descent(180/299): loss=100021.1491254773, w0=-0.8809932543791444, w1=-0.015662527040730068\n",
      "Gradient Descent(181/299): loss=100018.82746417329, w0=-0.8811351652311072, w1=-0.015008601433376118\n",
      "Gradient Descent(182/299): loss=100016.54079379325, w0=-0.8812758375457883, w1=-0.014358134904381663\n",
      "Gradient Descent(183/299): loss=100014.28851292771, w0=-0.8814152846082954, w1=-0.013711134482295039\n",
      "Gradient Descent(184/299): loss=100012.07003213905, w0=-0.881553519506754, w1=-0.013067606305762269\n",
      "Gradient Descent(185/299): loss=100009.8847736792, w0=-0.8816905551361712, w1=-0.012427555653159856\n",
      "Gradient Descent(186/299): loss=100007.73217121547, w0=-0.8818264042022089, w1=-0.01179098697130477\n",
      "Gradient Descent(187/299): loss=100005.61166956354, w0=-0.8819610792248681, w1=-0.011157903903271437\n",
      "Gradient Descent(188/299): loss=100003.52272442856, w0=-0.8820945925420872, w1=-0.010528309315344844\n",
      "Gradient Descent(189/299): loss=100001.46480215287, w0=-0.8822269563132572, w1=-0.009902205323138244\n",
      "Gradient Descent(190/299): loss=99999.43737947103, w0=-0.882358182522654, w1=-0.009279593316902649\n",
      "Gradient Descent(191/299): loss=99997.4399432714, w0=-0.8824882829827931, w1=-0.008660473986054906\n",
      "Gradient Descent(192/299): loss=99995.47199036421, w0=-0.8826172693377063, w1=-0.008044847342949855\n",
      "Gradient Descent(193/299): loss=99993.53302725623, w0=-0.8827451530661435, w1=-0.00743271274592165\n",
      "Gradient Descent(194/299): loss=99991.62256993097, w0=-0.8828719454847019, w1=-0.006824068921618376\n",
      "Gradient Descent(195/299): loss=99989.7401436354, w0=-0.8829976577508837, w1=-0.00621891398665303\n",
      "Gradient Descent(196/299): loss=99987.88528267191, w0=-0.883122300866085, w1=-0.005617245468593591\n",
      "Gradient Descent(197/299): loss=99986.0575301962, w0=-0.8832458856785178, w1=-0.005019060326313827\n",
      "Gradient Descent(198/299): loss=99984.25643802015, w0=-0.8833684228860655, w1=-0.004424354969725883\n",
      "Gradient Descent(199/299): loss=99982.4815664202, w0=-0.8834899230390758, w1=-0.003833125278914853\n",
      "Gradient Descent(200/299): loss=99980.73248395059, w0=-0.8836103965430903, w1=-0.00324536662269499\n",
      "Gradient Descent(201/299): loss=99979.00876726166, w0=-0.8837298536615145, w1=-0.0026610738766063282\n",
      "Gradient Descent(202/299): loss=99977.3100009225, w0=-0.8838483045182289, w1=-0.002080241440370078\n",
      "Gradient Descent(203/299): loss=99975.63577724881, w0=-0.8839657591001422, w1=-0.0015028632548201328\n",
      "Gradient Descent(204/299): loss=99973.98569613445, w0=-0.8840822272596882, w1=-0.0009289328183278075\n",
      "Gradient Descent(205/299): loss=99972.359364888, w0=-0.8841977187172696, w1=-0.00035844320273604015\n",
      "Gradient Descent(206/299): loss=99970.75639807316, w0=-0.8843122430636469, w1=0.00020861293118118582\n",
      "Gradient Descent(207/299): loss=99969.1764173531, w0=-0.8844258097622768, w1=0.0007722433187190616\n",
      "Gradient Descent(208/299): loss=99967.61905133902, w0=-0.8845384281515996, w1=0.0013324560766871222\n",
      "Gradient Descent(209/299): loss=99966.08393544241, w0=-0.8846501074472769, w1=0.0018892596892853917\n",
      "Gradient Descent(210/299): loss=99964.5707117309, w0=-0.8847608567443822, w1=0.0024426629943977455\n",
      "Gradient Descent(211/299): loss=99963.07902878805, w0=-0.8848706850195434, w1=0.002992675170289644\n",
      "Gradient Descent(212/299): loss=99961.6085415761, w0=-0.8849796011330404, w1=0.0035393057226973364\n",
      "Gradient Descent(213/299): loss=99960.15891130269, w0=-0.8850876138308583, w1=0.004082564472296605\n",
      "Gradient Descent(214/299): loss=99958.72980529034, w0=-0.8851947317466958, w1=0.004622461542539219\n",
      "Gradient Descent(215/299): loss=99957.32089684957, w0=-0.8853009634039324, w1=0.005159007347845763\n",
      "Gradient Descent(216/299): loss=99955.93186515503, w0=-0.8854063172175529, w1=0.0056922125821439774\n",
      "Gradient Descent(217/299): loss=99954.56239512403, w0=-0.8855108014960327, w1=0.006222088207742103\n",
      "Gradient Descent(218/299): loss=99953.21217729923, w0=-0.8856144244431822, w1=0.006748645444527085\n",
      "Gradient Descent(219/299): loss=99951.8809077329, w0=-0.8857171941599534, w1=0.0072718957594777416\n",
      "Gradient Descent(220/299): loss=99950.56828787476, w0=-0.8858191186462085, w1=0.007791850856483677\n",
      "Gradient Descent(221/299): loss=99949.27402446207, w0=-0.8859202058024515, w1=0.00830852266646058\n",
      "Gradient Descent(222/299): loss=99947.99782941282, w0=-0.8860204634315236, w1=0.008821923337753305\n",
      "Gradient Descent(223/299): loss=99946.73941972088, w0=-0.8861198992402646, w1=0.009332065226818117\n",
      "Gradient Descent(224/299): loss=99945.49851735389, w0=-0.8862185208411377, w1=0.009838960889175932\n",
      "Gradient Descent(225/299): loss=99944.27484915365, w0=-0.8863163357538234, w1=0.010342623070628696\n",
      "Gradient Descent(226/299): loss=99943.06814673878, w0=-0.8864133514067781, w1=0.010843064698731152\n",
      "Gradient Descent(227/299): loss=99941.8781464094, w0=-0.8865095751387622, w1=0.011340298874510596\n",
      "Gradient Descent(228/299): loss=99940.7045890544, w0=-0.8866050142003361, w1=0.011834338864427644\n",
      "Gradient Descent(229/299): loss=99939.54722006057, w0=-0.8866996757553263, w1=0.012325198092570875\n",
      "Gradient Descent(230/299): loss=99938.40578922404, w0=-0.8867935668822602, w1=0.012812890133078772\n",
      "Gradient Descent(231/299): loss=99937.28005066341, w0=-0.8868866945757732, w1=0.013297428702782534\n",
      "Gradient Descent(232/299): loss=99936.16976273543, w0=-0.8869790657479859, w1=0.013778827654063406\n",
      "Gradient Descent(233/299): loss=99935.07468795186, w0=-0.8870706872298539, w1=0.014257100967918575\n",
      "Gradient Descent(234/299): loss=99933.99459289886, w0=-0.8871615657724905, w1=0.01473226274722971\n",
      "Gradient Descent(235/299): loss=99932.92924815789, w0=-0.8872517080484617, w1=0.015204327210228451\n",
      "Gradient Descent(236/299): loss=99931.87842822823, w0=-0.8873411206530557, w1=0.01567330868415348\n",
      "Gradient Descent(237/299): loss=99930.84191145186, w0=-0.8874298101055272, w1=0.01613922159909367\n",
      "Gradient Descent(238/299): loss=99929.81947993921, w0=-0.8875177828503158, w1=0.016602080482012286\n",
      "Gradient Descent(239/299): loss=99928.81091949696, w0=-0.88760504525824, w1=0.017061899950947212\n",
      "Gradient Descent(240/299): loss=99927.81601955759, w0=-0.8876916036276683, w1=0.01751869470938238\n",
      "Gradient Descent(241/299): loss=99926.83457311019, w0=-0.8877774641856655, w1=0.01797247954078565\n",
      "Gradient Descent(242/299): loss=99925.86637663293, w0=-0.887862633089117, w1=0.018423269303308558\n",
      "Gradient Descent(243/299): loss=99924.91123002695, w0=-0.8879471164258305, w1=0.018871078924643632\n",
      "Gradient Descent(244/299): loss=99923.96893655162, w0=-0.8880309202156159, w1=0.01931592339703498\n",
      "Gradient Descent(245/299): loss=99923.03930276146, w0=-0.8881140504113434, w1=0.019757817772437854\n",
      "Gradient Descent(246/299): loss=99922.12213844419, w0=-0.888196512899981, w1=0.020196777157823222\n",
      "Gradient Descent(247/299): loss=99921.21725656006, w0=-0.8882783135036113, w1=0.020632816710623496\n",
      "Gradient Descent(248/299): loss=99920.32447318279, w0=-0.8883594579804285, w1=0.021065951634315506\n",
      "Gradient Descent(249/299): loss=99919.4436074414, w0=-0.8884399520257155, w1=0.02149619717413718\n",
      "Gradient Descent(250/299): loss=99918.57448146353, w0=-0.888519801272802, w1=0.021923568612934062\n",
      "Gradient Descent(251/299): loss=99917.71692032006, w0=-0.8885990112940036, w1=0.02234808126713249\n",
      "Gradient Descent(252/299): loss=99916.87075197027, w0=-0.8886775876015431, w1=0.02276975048283581\n",
      "Gradient Descent(253/299): loss=99916.03580720915, w0=-0.8887555356484532, w1=0.02318859163204051\n",
      "Gradient Descent(254/299): loss=99915.21191961489, w0=-0.8888328608294616, w1=0.023604620108968872\n",
      "Gradient Descent(255/299): loss=99914.39892549807, w0=-0.8889095684818595, w1=0.024017851326515238\n",
      "Gradient Descent(256/299): loss=99913.5966638515, w0=-0.8889856638863533, w1=0.02442830071280271\n",
      "Gradient Descent(257/299): loss=99912.80497630153, w0=-0.8890611522678985, w1=0.024835983707847387\n",
      "Gradient Descent(258/299): loss=99912.02370705991, w0=-0.8891360387965196, w1=0.0252409157603273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(259/299): loss=99911.25270287701, w0=-0.8892103285881126, w1=0.025643112324453248\n",
      "Gradient Descent(260/299): loss=99910.49181299587, w0=-0.8892840267052323, w1=0.026042588856938796\n",
      "Gradient Descent(261/299): loss=99909.7408891069, w0=-0.889357138157866, w1=0.026439360814066757\n",
      "Gradient Descent(262/299): loss=99908.99978530427, w0=-0.88942966790419, w1=0.026833443648849778\n",
      "Gradient Descent(263/299): loss=99908.26835804227, w0=-0.8895016208513138, w1=0.027224852808282275\n",
      "Gradient Descent(264/299): loss=99907.54646609319, w0=-0.8895730018560092, w1=0.027613603730681495\n",
      "Gradient Descent(265/299): loss=99906.83397050574, w0=-0.8896438157254256, w1=0.02799971184311519\n",
      "Gradient Descent(266/299): loss=99906.13073456453, w0=-0.8897140672177921, w1=0.028383192558913747\n",
      "Gradient Descent(267/299): loss=99905.4366237502, w0=-0.889783761043106, w1=0.028764061275264322\n",
      "Gradient Descent(268/299): loss=99904.75150570036, w0=-0.8898529018638087, w1=0.029142333370885044\n",
      "Gradient Descent(269/299): loss=99904.07525017145, w0=-0.889921494295449, w1=0.029518024203776947\n",
      "Gradient Descent(270/299): loss=99903.4077290011, w0=-0.8899895429073331, w1=0.02989114910905168\n",
      "Gradient Descent(271/299): loss=99902.7488160715, w0=-0.8900570522231641, w1=0.03026172339683294\n",
      "Gradient Descent(272/299): loss=99902.09838727348, w0=-0.8901240267216679, w1=0.030629762350229604\n",
      "Gradient Descent(273/299): loss=99901.45632047091, w0=-0.8901904708372087, w1=0.03099528122337881\n",
      "Gradient Descent(274/299): loss=99900.82249546655, w0=-0.8902563889603926, w1=0.03135829523955693\n",
      "Gradient Descent(275/299): loss=99900.1967939676, w0=-0.8903217854386605, w1=0.0317188195893567\n",
      "Gradient Descent(276/299): loss=99899.57909955287, w0=-0.8903866645768692, w1=0.03207686942892875\n",
      "Gradient Descent(277/299): loss=99898.96929764, w0=-0.8904510306378633, w1=0.03243245987828584\n",
      "Gradient Descent(278/299): loss=99898.36727545339, w0=-0.8905148878430352, w1=0.03278560601966801\n",
      "Gradient Descent(279/299): loss=99897.77292199302, w0=-0.8905782403728759, w1=0.03313632289596707\n",
      "Gradient Descent(280/299): loss=99897.18612800361, w0=-0.8906410923675152, w1=0.03348462550920888\n",
      "Gradient Descent(281/299): loss=99896.60678594447, w0=-0.8907034479272528, w1=0.03383052881909179\n",
      "Gradient Descent(282/299): loss=99896.03478995984, w0=-0.8907653111130791, w1=0.0341740477415797\n",
      "Gradient Descent(283/299): loss=99895.47003585006, w0=-0.8908266859471874, w1=0.03451519714754842\n",
      "Gradient Descent(284/299): loss=99894.912421043, w0=-0.890887576413476, w1=0.03485399186148367\n",
      "Gradient Descent(285/299): loss=99894.36184456604, w0=-0.8909479864580426, w1=0.035190446660229496\n",
      "Gradient Descent(286/299): loss=99893.81820701901, w0=-0.8910079199896684, w1=0.03552457627178564\n",
      "Gradient Descent(287/299): loss=99893.28141054706, w0=-0.8910673808802952, w1=0.035856395374152555\n",
      "Gradient Descent(288/299): loss=99892.75135881425, w0=-0.8911263729654927, w1=0.03618591859422271\n",
      "Gradient Descent(289/299): loss=99892.22795697807, w0=-0.8911849000449186, w1=0.03651316050671708\n",
      "Gradient Descent(290/299): loss=99891.71111166364, w0=-0.8912429658827701, w1=0.03683813563316539\n",
      "Gradient Descent(291/299): loss=99891.20073093908, w0=-0.8913005742082277, w1=0.037160858440929034\n",
      "Gradient Descent(292/299): loss=99890.69672429103, w0=-0.8913577287158914, w1=0.03748134334226545\n",
      "Gradient Descent(293/299): loss=99890.19900260071, w0=-0.8914144330662087, w1=0.03779960469343278\n",
      "Gradient Descent(294/299): loss=99889.70747812027, w0=-0.8914706908858964, w1=0.03811565679383373\n",
      "Gradient Descent(295/299): loss=99889.2220644498, w0=-0.8915265057683538, w1=0.038429513885197615\n",
      "Gradient Descent(296/299): loss=99888.7426765149, w0=-0.8915818812740702, w1=0.03874119015079942\n",
      "Gradient Descent(297/299): loss=99888.26923054399, w0=-0.891636820931024, w1=0.03905069971471484\n",
      "Gradient Descent(298/299): loss=99887.80164404679, w0=-0.8916913282350764, w1=0.039358056641110375\n",
      "Gradient Descent(299/299): loss=99887.33983579294, w0=-0.8917454066503572, w1=0.03966327493356748\n",
      "Gradient Descent(0/299): loss=138629.43611198498, w0=-0.15765500000000002, w1=0.004490149132373062\n",
      "Gradient Descent(1/299): loss=122165.05975406554, w0=-0.27605183720793375, w1=0.007210503125143516\n",
      "Gradient Descent(2/299): loss=115397.13579186924, w0=-0.36936465122981776, w1=0.008997445658809238\n",
      "Gradient Descent(3/299): loss=111829.31688167981, w0=-0.44362911250606224, w1=0.008140327659560005\n",
      "Gradient Descent(4/299): loss=109616.14358680017, w0=-0.5035888682395014, w1=0.005830820321160803\n",
      "Gradient Descent(5/299): loss=108134.68606551818, w0=-0.5525418796086248, w1=0.002625991467351064\n",
      "Gradient Descent(6/299): loss=107088.91506064146, w0=-0.592868143985939, w1=-0.0010682829748993087\n",
      "Gradient Descent(7/299): loss=106318.76595462534, w0=-0.6263500008676256, w1=-0.005011701134892549\n",
      "Gradient Descent(8/299): loss=105730.78971046177, w0=-0.6543425284571024, w1=-0.00905591910086307\n",
      "Gradient Descent(9/299): loss=105267.51068488075, w0=-0.6778936286397133, w1=-0.013110025166117435\n",
      "Gradient Descent(10/299): loss=104892.16492002625, w0=-0.6978234208485714, w1=-0.017117809800215007\n",
      "Gradient Descent(11/299): loss=104580.48940570408, w0=-0.7147804855140841, w1=-0.021044406840767443\n",
      "Gradient Descent(12/299): loss=104316.0466408821, w0=-0.7292823187586323, w1=-0.024868233420749723\n",
      "Gradient Descent(13/299): loss=104087.4441055202, w0=-0.7417450705863098, w1=-0.02857610819528111\n",
      "Gradient Descent(14/299): loss=103886.62264524637, w0=-0.7525057432219181, w1=-0.03216025587475007\n",
      "Gradient Descent(15/299): loss=103707.77271375382, w0=-0.7618389800102994, w1=-0.035616443046441335\n",
      "Gradient Descent(16/299): loss=103546.63168443344, w0=-0.769969904723564, w1=-0.03894279896246321\n",
      "Gradient Descent(17/299): loss=103400.01882322281, w0=-0.7770840358971416, w1=-0.04213905597299812\n",
      "Gradient Descent(18/299): loss=103265.52194874553, w0=-0.7833350087055552, w1=-0.04520605054099522\n",
      "Gradient Descent(19/299): loss=103141.28286622866, w0=-0.7888506361995006, w1=-0.04814538858274276\n",
      "Gradient Descent(20/299): loss=103025.84826621933, w0=-0.7937377010702354, w1=-0.050959216243011723\n",
      "Gradient Descent(21/299): loss=102918.0647030412, w0=-0.7980857688845886, w1=-0.053650059592149725\n",
      "Gradient Descent(22/299): loss=102817.00368169762, w0=-0.8019702413187556, w1=-0.05622071024592265\n",
      "Gradient Descent(23/299): loss=102721.90758166388, w0=-0.8054548149692018, w1=-0.05867414216063152\n",
      "Gradient Descent(24/299): loss=102632.15017714433, w0=-0.808593472199907, w1=-0.061013449971774075\n",
      "Gradient Descent(25/299): loss=102547.2074989086, w0=-0.8114321013192012, w1=-0.06324180247583164\n",
      "Gradient Descent(26/299): loss=102466.6361019029, w0=-0.8140098214549649, w1=-0.06536240693670159\n",
      "Gradient Descent(27/299): loss=102390.05669049753, w0=-0.8163600708918252, w1=-0.06737848126347784\n",
      "Gradient Descent(28/299): loss=102317.1416576975, w0=-0.818511504969549, w1=-0.06929323201246673\n",
      "Gradient Descent(29/299): loss=102247.6055107718, w0=-0.8204887399191128, w1=-0.07110983677117985\n",
      "Gradient Descent(30/299): loss=102181.19744511324, w0=-0.8223129715011558, w1=-0.07283142988592989\n",
      "Gradient Descent(31/299): loss=102117.69553115478, w0=-0.8240024914712211, w1=-0.07446109076463794\n",
      "Gradient Descent(32/299): loss=102056.90212268537, w0=-0.8255731203251081, w1=-0.07600183416979683\n",
      "Gradient Descent(33/299): loss=101998.64019690902, w0=-0.827038571172576, w1=-0.07745660204848019\n",
      "Gradient Descent(34/299): loss=101942.75040923314, w0=-0.8284107567170047, w1=-0.07882825655480113\n",
      "Gradient Descent(35/299): loss=101889.08869740058, w0=-0.8297000490038333, w1=-0.08011957402855115\n",
      "Gradient Descent(36/299): loss=101837.52430612102, w0=-0.8309154997044217, w1=-0.08133323982063459\n",
      "Gradient Descent(37/299): loss=101787.93812941146, w0=-0.8320650271243114, w1=-0.082471844012441\n",
      "Gradient Descent(38/299): loss=101740.22128753315, w0=-0.8331555748054562, w1=-0.08353787825847521\n",
      "Gradient Descent(39/299): loss=101694.27387284473, w0=-0.834193245517377, w1=-0.08453373415996449\n",
      "Gradient Descent(40/299): loss=101650.00381797252, w0=-0.8351834136373807, w1=-0.08546170369010951\n",
      "Gradient Descent(41/299): loss=101607.3258627473, w0=-0.8361308184690115, w1=-0.08632398215481536\n",
      "Gradient Descent(42/299): loss=101566.16062178239, w0=-0.8370396409800591, w1=-0.08712267391731453\n",
      "Gradient Descent(43/299): loss=101526.43377560409, w0=-0.8379135666949168, w1=-0.08785980064820206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/299): loss=101488.07541534092, w0=-0.8387558378415672, w1=-0.08853731130996724\n",
      "Gradient Descent(45/299): loss=101451.01955857131, w0=-0.8395692980236372, w1=-0.0891570926623874\n",
      "Gradient Descent(46/299): loss=101415.2038268479, w0=-0.8403564324090721, w1=-0.08972097897448165\n",
      "Gradient Descent(47/299): loss=101380.56924772337, w0=-0.8411194056571295, w1=-0.09023075989732388\n",
      "Gradient Descent(48/299): loss=101347.0601297606, w0=-0.841860098749864, w1=-0.09068818596240288\n",
      "Gradient Descent(49/299): loss=101314.62396257307, w0=-0.8425801448826462, w1=-0.09109497170845152\n",
      "Gradient Descent(50/299): loss=101283.21130962271, w0=-0.8432809638712306, w1=-0.09145279683194937\n",
      "Gradient Descent(51/299): loss=101252.77567965169, w0=-0.8439637942494015, w1=-0.09176330593822156\n",
      "Gradient Descent(52/299): loss=101223.27337628663, w0=-0.8446297222961088, w1=-0.09202810747269054\n",
      "Gradient Descent(53/299): loss=101194.66333248545, w0=-0.8452797074971683, w1=-0.0922487723091402\n",
      "Gradient Descent(54/299): loss=101166.90693836565, w0=-0.845914604269284, w1=-0.09242683233323121\n",
      "Gradient Descent(55/299): loss=101139.96786988221, w0=-0.8465351800562565, w1=-0.0925637792295167\n",
      "Gradient Descent(56/299): loss=101113.81192368129, w0=-0.8471421301054235, w1=-0.0926610635785233\n",
      "Gradient Descent(57/299): loss=101088.40686135, w0=-0.8477360893423282, w1=-0.09272009430026135\n",
      "Gradient Descent(58/299): loss=101063.72226462723, w0=-0.8483176417993423, w1=-0.09274223843701797\n",
      "Gradient Descent(59/299): loss=101039.72940201941, w0=-0.8488873280417005, w1=-0.09272882124436847\n",
      "Gradient Descent(60/299): loss=101016.40110657264, w0=-0.8494456509926692, w1=-0.09268112654845313\n",
      "Gradient Descent(61/299): loss=100993.71166418155, w0=-0.8499930805040944, w1=-0.09260039732466233\n",
      "Gradient Descent(62/299): loss=100971.6367116333, w0=-0.8505300569599357, w1=-0.09248783645446934\n",
      "Gradient Descent(63/299): loss=100950.15314353847, w0=-0.8510569941449013, w1=-0.09234460762101541\n",
      "Gradient Descent(64/299): loss=100929.23902731297, w0=-0.8515742815611632, w1=-0.09217183630884711\n",
      "Gradient Descent(65/299): loss=100908.87352542399, w0=-0.8520822863345384, w1=-0.09197061087816678\n",
      "Gradient Descent(66/299): loss=100889.0368241798, w0=-0.852581354817456, w1=-0.0917419836886849\n",
      "Gradient Descent(67/299): loss=100869.71006841023, w0=-0.8530718139688271, w1=-0.0914869722524644\n",
      "Gradient Descent(68/299): loss=100850.87530145368, w0=-0.853553972569666, w1=-0.09120656039895324\n",
      "Gradient Descent(69/299): loss=100832.51540992557, w0=-0.8540281223169812, w1=-0.090901699438703\n",
      "Gradient Descent(70/299): loss=100814.61407280514, w0=-0.8544945388261223, w1=-0.09057330931509622\n",
      "Gradient Descent(71/299): loss=100797.15571442244, w0=-0.854953482562604, w1=-0.0902222797357944\n",
      "Gradient Descent(72/299): loss=100780.1254609793, w0=-0.8554051997177317, w1=-0.08984947127761812\n",
      "Gradient Descent(73/299): loss=100763.50910027273, w0=-0.8558499230375471, w1=-0.0894557164602294\n",
      "Gradient Descent(74/299): loss=100747.29304432822, w0=-0.8562878726112344, w1=-0.08904182078534931\n",
      "Gradient Descent(75/299): loss=100731.46429468135, w0=-0.8567192566228136, w1=-0.08860856373935269\n",
      "Gradient Descent(76/299): loss=100716.01041007388, w0=-0.8571442720684169, w1=-0.08815669975797454\n",
      "Gradient Descent(77/299): loss=100700.9194763536, w0=-0.8575631054404717, w1=-0.08768695915257468\n",
      "Gradient Descent(78/299): loss=100686.18007839195, w0=-0.8579759333795449, w1=-0.08720004899796628\n",
      "Gradient Descent(79/299): loss=100671.78127385021, w0=-0.8583829232943115, w1=-0.08669665398224663\n",
      "Gradient Descent(80/299): loss=100657.71256864288, w0=-0.858784233949994, w1=-0.08617743721939926\n",
      "Gradient Descent(81/299): loss=100643.96389396218, w0=-0.8591800160256333, w1=-0.08564304102567974\n",
      "Gradient Descent(82/299): loss=100630.52558474077, w0=-0.8595704126406261, w1=-0.08509408766097516\n",
      "Gradient Descent(83/299): loss=100617.38835944125, w0=-0.85995555985108, w1=-0.08453118003644683\n",
      "Gradient Descent(84/299): loss=100604.54330107252, w0=-0.8603355871166666, w1=-0.08395490238984325\n",
      "Gradient Descent(85/299): loss=100591.98183934105, w0=-0.8607106177387783, w1=-0.08336582092991264\n",
      "Gradient Descent(86/299): loss=100579.69573385498, w0=-0.8610807692709141, w1=-0.08276448445135853\n",
      "Gradient Descent(87/299): loss=100567.67705830482, w0=-0.861446153902315, w1=-0.08215142492177685\n",
      "Gradient Descent(88/299): loss=100555.91818555298, w0=-0.861806878815955, w1=-0.08152715804199025\n",
      "Gradient Descent(89/299): loss=100544.41177356835, w0=-0.8621630465220492, w1=-0.0808921837811619\n",
      "Gradient Descent(90/299): loss=100533.15075214917, w0=-0.8625147551682875, w1=-0.08024698688802882\n",
      "Gradient Descent(91/299): loss=100522.12831038068, w0=-0.8628620988280196, w1=-0.07959203737954547\n",
      "Gradient Descent(92/299): loss=100511.33788478021, w0=-0.8632051677676308, w1=-0.07892779100817747\n",
      "Gradient Descent(93/299): loss=100500.77314808426, w0=-0.8635440486943352, w1=-0.07825468970902844\n",
      "Gradient Descent(94/299): loss=100490.4279986372, w0=-0.8638788249856001, w1=-0.07757316202792917\n",
      "Gradient Descent(95/299): loss=100480.29655034324, w0=-0.8642095769013823, w1=-0.07688362353156042\n",
      "Gradient Descent(96/299): loss=100470.37312314664, w0=-0.8645363817803243, w1=-0.07618647720062645\n",
      "Gradient Descent(97/299): loss=100460.652234009, w0=-0.8648593142210133, w1=-0.07548211380704099\n",
      "Gradient Descent(98/299): loss=100451.12858835136, w0=-0.8651784462493616, w1=-0.07477091227603563\n",
      "Gradient Descent(99/299): loss=100441.79707193581, w0=-0.8654938474731152, w1=-0.07405324003404797\n",
      "Gradient Descent(100/299): loss=100432.65274315895, w0=-0.865805585224445, w1=-0.07332945334319925\n",
      "Gradient Descent(101/299): loss=100423.69082573362, w0=-0.8661137246915229, w1=-0.07259989762312315\n",
      "Gradient Descent(102/299): loss=100414.9067017369, w0=-0.866418329039933, w1=-0.07186490776086252\n",
      "Gradient Descent(103/299): loss=100406.29590500306, w0=-0.8667194595247107, w1=-0.07112480840950866\n",
      "Gradient Descent(104/299): loss=100397.8541148422, w0=-0.8670171755937578, w1=-0.07037991427621589\n",
      "Gradient Descent(105/299): loss=100389.57715006583, w0=-0.8673115349833236, w1=-0.06963053040018662\n",
      "Gradient Descent(106/299): loss=100381.46096330337, w0=-0.8676025938062001, w1=-0.06887695242118416\n",
      "Gradient Descent(107/299): loss=100373.50163559236, w0=-0.8678904066332294, w1=-0.06811946683909659\n",
      "Gradient Descent(108/299): loss=100365.6953712286, w0=-0.8681750265686774, w1=-0.06735835126504125\n",
      "Gradient Descent(109/299): loss=100358.03849286145, w0=-0.8684565053199877, w1=-0.06659387466446838\n",
      "Gradient Descent(110/299): loss=100350.52743682088, w0=-0.8687348932623875, w1=-0.06582629759269223\n",
      "Gradient Descent(111/299): loss=100343.1587486655, w0=-0.8690102394987821, w1=-0.06505587242325027\n",
      "Gradient Descent(112/299): loss=100335.92907893758, w0=-0.8692825919153391, w1=-0.0642828435694639\n",
      "Gradient Descent(113/299): loss=100328.83517911605, w0=-0.8695519972331299, w1=-0.06350744769954839\n",
      "Gradient Descent(114/299): loss=100321.87389775632, w0=-0.869818501056167, w1=-0.06272991394559643\n",
      "Gradient Descent(115/299): loss=100315.04217680736, w0=-0.8700821479161462, w1=-0.061950464106736054\n",
      "Gradient Descent(116/299): loss=100308.3370480971, w0=-0.8703429813141778, w1=-0.06116931284674223\n",
      "Gradient Descent(117/299): loss=100301.75562997698, w0=-0.8706010437597649, w1=-0.060386667886361094\n",
      "Gradient Descent(118/299): loss=100295.2951241181, w0=-0.8708563768072669, w1=-0.05960273019058618\n",
      "Gradient Descent(119/299): loss=100288.9528124512, w0=-0.871109021090063, w1=-0.05881769415110732\n",
      "Gradient Descent(120/299): loss=100282.72605424264, w0=-0.8713590163526129, w1=-0.05803174776413652\n",
      "Gradient Descent(121/299): loss=100276.61228330032, w0=-0.871606401480595, w1=-0.057245072803797696\n",
      "Gradient Descent(122/299): loss=100270.60900530222, w0=-0.8718512145292839, w1=-0.056457844991252684\n",
      "Gradient Descent(123/299): loss=100264.71379524245, w0=-0.8720934927503182, w1=-0.055670234159721256\n",
      "Gradient Descent(124/299): loss=100258.92429498768, w0=-0.8723332726169901, w1=-0.05488240441553974\n",
      "Gradient Descent(125/299): loss=100253.23821093954, w0=-0.8725705898481829, w1=-0.054094514295390084\n",
      "Gradient Descent(126/299): loss=100247.65331179736, w0=-0.872805479431066, w1=-0.053306716919820085\n",
      "Gradient Descent(127/299): loss=100242.16742641618, w0=-0.873037975642648, w1=-0.052519160143164795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(128/299): loss=100236.77844175571, w0=-0.8732681120702808, w1=-0.051731986699969305\n",
      "Gradient Descent(129/299): loss=100231.48430091521, w0=-0.8734959216311964, w1=-0.050945334348004225\n",
      "Gradient Descent(130/299): loss=100226.28300125067, w0=-0.8737214365911523, w1=-0.05015933600795756\n",
      "Gradient Descent(131/299): loss=100221.17259257047, w0=-0.8739446885822528, w1=-0.04937411989987871\n",
      "Gradient Descent(132/299): loss=100216.15117540461, w0=-0.874165708620008, w1=-0.04858980967644482\n",
      "Gradient Descent(133/299): loss=100211.21689934503, w0=-0.8743845271196863, w1=-0.047806524553113135\n",
      "Gradient Descent(134/299): loss=100206.36796145374, w0=-0.8746011739120108, w1=-0.0470243794352185\n",
      "Gradient Descent(135/299): loss=100201.60260473414, w0=-0.8748156782582442, w1=-0.046243485042070716\n",
      "Gradient Descent(136/299): loss=100196.91911666428, w0=-0.875028068864705, w1=-0.04546394802810266\n",
      "Gradient Descent(137/299): loss=100192.31582778762, w0=-0.8752383738967492, w1=-0.04468587110111717\n",
      "Gradient Descent(138/299): loss=100187.79111035982, w0=-0.8754466209922557, w1=-0.04390935313767773\n",
      "Gradient Descent(139/299): loss=100183.34337704777, w0=-0.8756528372746414, w1=-0.04313448929568632\n",
      "Gradient Descent(140/299): loss=100178.97107967945, w0=-0.8758570493654363, w1=-0.04236137112419\n",
      "Gradient Descent(141/299): loss=100174.67270804162, w0=-0.8760592833964427, w1=-0.041590086670456265\n",
      "Gradient Descent(142/299): loss=100170.44678872317, w0=-0.8762595650215012, w1=-0.04082072058435709\n",
      "Gradient Descent(143/299): loss=100166.29188400236, w0=-0.8764579194278849, w1=-0.040053354220100305\n",
      "Gradient Descent(144/299): loss=100162.20659077587, w0=-0.8766543713473394, w1=-0.03928806573534708\n",
      "Gradient Descent(145/299): loss=100158.18953952714, w0=-0.8768489450667866, w1=-0.038524930187754165\n",
      "Gradient Descent(146/299): loss=100154.23939333324, w0=-0.8770416644387082, w1=-0.037764019628979954\n",
      "Gradient Descent(147/299): loss=100150.35484690795, w0=-0.8772325528912227, w1=-0.03700540319619327\n",
      "Gradient Descent(148/299): loss=100146.53462567936, w0=-0.8774216334378687, w1=-0.03624914720112487\n",
      "Gradient Descent(149/299): loss=100142.77748490055, w0=-0.8776089286871085, w1=-0.03549531521670182\n",
      "Gradient Descent(150/299): loss=100139.0822087925, w0=-0.8777944608515613, w1=-0.03474396816130575\n",
      "Gradient Descent(151/299): loss=100135.44760971666, w0=-0.8779782517569775, w1=-0.033995164380696455\n",
      "Gradient Descent(152/299): loss=100131.8725273771, w0=-0.8781603228509633, w1=-0.03324895972764356\n",
      "Gradient Descent(153/299): loss=100128.35582804988, w0=-0.8783406952114657, w1=-0.032505407639308675\n",
      "Gradient Descent(154/299): loss=100124.89640383994, w0=-0.878519389555025, w1=-0.0317645592124223\n",
      "Gradient Descent(155/299): loss=100121.49317196199, w0=-0.8786964262448036, w1=-0.031026463276299612\n",
      "Gradient Descent(156/299): loss=100118.14507404764, w0=-0.8788718252983992, w1=-0.030291166463739885\n",
      "Gradient Descent(157/299): loss=100114.85107547432, w0=-0.8790456063954483, w1=-0.029558713279855132\n",
      "Gradient Descent(158/299): loss=100111.61016471777, w0=-0.879217788885027, w1=-0.02882914616887398\n",
      "Gradient Descent(159/299): loss=100108.42135272521, w0=-0.8793883917928575, w1=-0.0281025055789665\n",
      "Gradient Descent(160/299): loss=100105.28367231008, w0=-0.8795574338283236, w1=-0.027378830025137125\n",
      "Gradient Descent(161/299): loss=100102.19617756575, w0=-0.8797249333913028, w1=-0.026658156150231666\n",
      "Gradient Descent(162/299): loss=100099.1579432989, w0=-0.8798909085788202, w1=-0.02594051878410563\n",
      "Gradient Descent(163/299): loss=100096.16806448081, w0=-0.8800553771915309, w1=-0.02522595100100029\n",
      "Gradient Descent(164/299): loss=100093.22565571652, w0=-0.8802183567400332, w1=-0.024514484175173275\n",
      "Gradient Descent(165/299): loss=100090.32985073091, w0=-0.8803798644510207, w1=-0.023806148034829992\n",
      "Gradient Descent(166/299): loss=100087.47980187066, w0=-0.8805399172732765, w1=-0.02310097071440212\n",
      "Gradient Descent(167/299): loss=100084.67467962239, w0=-0.880698531883515, w1=-0.02239897880521877\n",
      "Gradient Descent(168/299): loss=100081.91367214538, w0=-0.8808557246920747, w1=-0.02170019740461593\n",
      "Gradient Descent(169/299): loss=100079.19598481912, w0=-0.8810115118484686, w1=-0.02100465016352879\n",
      "Gradient Descent(170/299): loss=100076.52083980486, w0=-0.8811659092467929, w1=-0.020312359332611152\n",
      "Gradient Descent(171/299): loss=100073.88747562007, w0=-0.8813189325310029, w1=-0.019623345806925638\n",
      "Gradient Descent(172/299): loss=100071.29514672658, w0=-0.8814705971000549, w1=-0.018937629169247572\n",
      "Gradient Descent(173/299): loss=100068.74312313074, w0=-0.8816209181129225, w1=-0.018255227732024423\n",
      "Gradient Descent(174/299): loss=100066.23068999557, w0=-0.8817699104934882, w1=-0.017576158578032398\n",
      "Gradient Descent(175/299): loss=100063.75714726497, w0=-0.8819175889353147, w1=-0.016900437599770505\n",
      "Gradient Descent(176/299): loss=100061.32180929862, w0=-0.8820639679062998, w1=-0.016228079537631605\n",
      "Gradient Descent(177/299): loss=100058.92400451793, w0=-0.8822090616532183, w1=-0.015559098016889266\n",
      "Gradient Descent(178/299): loss=100056.5630750624, w0=-0.8823528842061534, w1=-0.014893505583538125\n",
      "Gradient Descent(179/299): loss=100054.23837645598, w0=-0.8824954493828214, w1=-0.014231313739024424\n",
      "Gradient Descent(180/299): loss=100051.94927728313, w0=-0.882636770792793, w1=-0.0135725329739028\n",
      "Gradient Descent(181/299): loss=100049.69515887453, w0=-0.8827768618416137, w1=-0.012917172800453954\n",
      "Gradient Descent(182/299): loss=100047.47541500116, w0=-0.8829157357348265, w1=-0.012265241784297144\n",
      "Gradient Descent(183/299): loss=100045.28945157811, w0=-0.8830534054818991, w1=-0.011616747575030456\n",
      "Gradient Descent(184/299): loss=100043.1366863759, w0=-0.8831898839000594, w1=-0.010971696935930825\n",
      "Gradient Descent(185/299): loss=100041.01654874068, w0=-0.8833251836180407, w1=-0.010330095772744446\n",
      "Gradient Descent(186/299): loss=100038.92847932191, w0=-0.8834593170797402, w1=-0.009691949161597994\n",
      "Gradient Descent(187/299): loss=100036.87192980776, w0=-0.8835922965477926, w1=-0.009057261376059387\n",
      "Gradient Descent(188/299): loss=100034.84636266818, w0=-0.8837241341070607, w1=-0.008426035913376246\n",
      "Gradient Descent(189/299): loss=100032.85125090452, w0=-0.883854841668047, w1=-0.007798275519919245\n",
      "Gradient Descent(190/299): loss=100030.8860778069, w0=-0.8839844309702259, w1=-0.0071739822158565355\n",
      "Gradient Descent(191/299): loss=100028.95033671739, w0=-0.8841129135853018, w1=-0.006553157319084743\n",
      "Gradient Descent(192/299): loss=100027.04353080023, w0=-0.8842403009203915, w1=-0.005935801468440899\n",
      "Gradient Descent(193/299): loss=100025.16517281818, w0=-0.884366604221137, w1=-0.005321914646219022\n",
      "Gradient Descent(194/299): loss=100023.31478491484, w0=-0.884491834574746, w1=-0.004711496200014051\n",
      "Gradient Descent(195/299): loss=100021.49189840289, w0=-0.8846160029129668, w1=-0.004104544863915313\n",
      "Gradient Descent(196/299): loss=100019.69605355816, w0=-0.8847391200149951, w1=-0.003501058779070594\n",
      "Gradient Descent(197/299): loss=100017.92679941887, w0=-0.8848611965103174, w1=-0.0029010355136412907\n",
      "Gradient Descent(198/299): loss=100016.18369359054, w0=-0.8849822428814909, w1=-0.0023044720821685074\n",
      "Gradient Descent(199/299): loss=100014.46630205605, w0=-0.8851022694668629, w1=-0.0017113649643688967\n",
      "Gradient Descent(200/299): loss=100012.77419899014, w0=-0.88522128646323, w1=-0.0011217101233788232\n",
      "Gradient Descent(201/299): loss=100011.10696657973, w0=-0.8853393039284391, w1=-0.0005355030234641791\n",
      "Gradient Descent(202/299): loss=100009.46419484826, w0=-0.8854563317839323, w1=4.726135278691713e-05\n",
      "Gradient Descent(203/299): loss=100007.8454814844, w0=-0.8855723798172362, w1=0.0006265884877720508\n",
      "Gradient Descent(204/299): loss=100006.25043167605, w0=-0.885687457684397, w1=0.001202484312667651\n",
      "Gradient Descent(205/299): loss=100004.67865794775, w0=-0.8858015749123638, w1=0.0017749551916917264\n",
      "Gradient Descent(206/299): loss=100003.1297800027, w0=-0.8859147409013198, w1=0.00234400790682188\n",
      "Gradient Descent(207/299): loss=100001.6034245687, w0=-0.8860269649269644, w1=0.0029096496429540887\n",
      "Gradient Descent(208/299): loss=100000.09922524804, w0=-0.8861382561427456, w1=0.0034718879734890714\n",
      "Gradient Descent(209/299): loss=99998.61682237091, w0=-0.8862486235820451, w1=0.004030730846332835\n",
      "Gradient Descent(210/299): loss=99997.15586285318, w0=-0.8863580761603177, w1=0.00458618657029877\n",
      "Gradient Descent(211/299): loss=99995.71600005674, w0=-0.8864666226771836, w1=0.005138263801899272\n",
      "Gradient Descent(212/299): loss=99994.29689365425, w0=-0.8865742718184779, w1=0.00568697153251497\n",
      "Gradient Descent(213/299): loss=99992.89820949666, w0=-0.8866810321582563, w1=0.006232319075930382\n",
      "Gradient Descent(214/299): loss=99991.51961948443, w0=-0.8867869121607583, w1=0.006774316056224754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(215/299): loss=99990.16080144112, w0=-0.8868919201823295, w1=0.007312972396007972\n",
      "Gradient Descent(216/299): loss=99988.82143899138, w0=-0.8869960644733033, w1=0.00784829830499106\n",
      "Gradient Descent(217/299): loss=99987.50122144059, w0=-0.8870993531798432, w1=0.008380304268881587\n",
      "Gradient Descent(218/299): loss=99986.19984365825, w0=-0.8872017943457471, w1=0.008909001038594388\n",
      "Gradient Descent(219/299): loss=99984.9170059638, w0=-0.887303395914213, w1=0.009434399619768427\n",
      "Gradient Descent(220/299): loss=99983.65241401533, w0=-0.8874041657295693, w1=0.009956511262581262\n",
      "Gradient Descent(221/299): loss=99982.40577870101, w0=-0.8875041115389675, w1=0.010475347451852184\n",
      "Gradient Descent(222/299): loss=99981.17681603292, w0=-0.8876032409940415, w1=0.010990919897425839\n",
      "Gradient Descent(223/299): loss=99979.96524704348, w0=-0.8877015616525314, w1=0.01150324052482872\n",
      "Gradient Descent(224/299): loss=99978.77079768444, w0=-0.8877990809798743, w1=0.01201232146619031\n",
      "Gradient Descent(225/299): loss=99977.59319872814, w0=-0.8878958063507616, w1=0.012518175051421792\n",
      "Gradient Descent(226/299): loss=99976.43218567094, w0=-0.8879917450506657, w1=0.0130208137996448\n",
      "Gradient Descent(227/299): loss=99975.28749863926, w0=-0.8880869042773337, w1=0.013520250410863648\n",
      "Gradient Descent(228/299): loss=99974.15888229746, w0=-0.8881812911422516, w1=0.014016497757873758\n",
      "Gradient Descent(229/299): loss=99973.046085758, w0=-0.8882749126720777, w1=0.014509568878400207\n",
      "Gradient Descent(230/299): loss=99971.9488624935, w0=-0.8883677758100473, w1=0.014999476967459879\n",
      "Gradient Descent(231/299): loss=99970.86697025143, w0=-0.8884598874173489, w1=0.015486235369940742\n",
      "Gradient Descent(232/299): loss=99969.8001709696, w0=-0.8885512542744715, w1=0.015969857573393156\n",
      "Gradient Descent(233/299): loss=99968.74823069492, w0=-0.888641883082525, w1=0.016450357201026503\n",
      "Gradient Descent(234/299): loss=99967.71091950282, w0=-0.8887317804645344, w1=0.016927748004906117\n",
      "Gradient Descent(235/299): loss=99966.6880114192, w0=-0.8888209529667066, w1=0.01740204385934506\n",
      "Gradient Descent(236/299): loss=99965.67928434398, w0=-0.8889094070596725, w1=0.017873258754485248\n",
      "Gradient Descent(237/299): loss=99964.68451997619, w0=-0.8889971491397036, w1=0.018341406790063037\n",
      "Gradient Descent(238/299): loss=99963.70350374092, w0=-0.8890841855299042, w1=0.018806502169354186\n",
      "Gradient Descent(239/299): loss=99962.73602471774, w0=-0.8891705224813795, w1=0.019268559193293684\n",
      "Gradient Descent(240/299): loss=99961.78187557086, w0=-0.8892561661743804, w1=0.01972759225476533\n",
      "Gradient Descent(241/299): loss=99960.84085248073, w0=-0.8893411227194252, w1=0.020183615833056924\n",
      "Gradient Descent(242/299): loss=99959.91275507706, w0=-0.889425398158399, w1=0.020636644488476477\n",
      "Gradient Descent(243/299): loss=99958.99738637349, w0=-0.8895089984656309, w1=0.021086692857125295\n",
      "Gradient Descent(244/299): loss=99958.09455270352, w0=-0.8895919295489503, w1=0.021533775645823733\n",
      "Gradient Descent(245/299): loss=99957.20406365804, w0=-0.889674197250721, w1=0.021977907627185503\n",
      "Gradient Descent(246/299): loss=99956.3257320239, w0=-0.8897558073488567, w1=0.022419103634836842\n",
      "Gradient Descent(247/299): loss=99955.45937372388, w0=-0.889836765557814, w1=0.022857378558776446\n",
      "Gradient Descent(248/299): loss=99954.60480775847, w0=-0.8899170775295676, w1=0.023292747340872742\n",
      "Gradient Descent(249/299): loss=99953.76185614793, w0=-0.8899967488545657, w1=0.02372522497049482\n",
      "Gradient Descent(250/299): loss=99952.93034387653, w0=-0.890075785062666, w1=0.024154826480273305\n",
      "Gradient Descent(251/299): loss=99952.11009883713, w0=-0.8901541916240546, w1=0.02458156694198809\n",
      "Gradient Descent(252/299): loss=99951.30095177774, w0=-0.8902319739501451, w1=0.02500546146257945\n",
      "Gradient Descent(253/299): loss=99950.50273624877, w0=-0.8903091373944617, w1=0.025426525180279318\n",
      "Gradient Descent(254/299): loss=99949.7152885511, w0=-0.8903856872535044, w1=0.025844773260859627\n",
      "Gradient Descent(255/299): loss=99948.93844768599, w0=-0.8904616287675972, w1=0.02626022089399455\n",
      "Gradient Descent(256/299): loss=99948.17205530527, w0=-0.8905369671217198, w1=0.02667288328973394\n",
      "Gradient Descent(257/299): loss=99947.41595566322, w0=-0.890611707446324, w1=0.02708277567508465\n",
      "Gradient Descent(258/299): loss=99946.66999556884, w0=-0.8906858548181329, w1=0.027489913290697346\n",
      "Gradient Descent(259/299): loss=99945.9340243396, w0=-0.8907594142609259, w1=0.027894311387655895\n",
      "Gradient Descent(260/299): loss=99945.2078937559, w0=-0.8908323907463078, w1=0.02829598522436641\n",
      "Gradient Descent(261/299): loss=99944.49145801639, w0=-0.8909047891944638, w1=0.028694950063543855\n",
      "Gradient Descent(262/299): loss=99943.78457369449, w0=-0.8909766144748992, w1=0.029091221169293222\n",
      "Gradient Descent(263/299): loss=99943.08709969545, w0=-0.8910478714071657, w1=0.029484813804283055\n",
      "Gradient Descent(264/299): loss=99942.39889721459, w0=-0.8911185647615737, w1=0.029875743227008863\n",
      "Gradient Descent(265/299): loss=99941.71982969616, w0=-0.8911886992598907, w1=0.03026402468914397\n",
      "Gradient Descent(266/299): loss=99941.04976279305, w0=-0.891258279576027, w1=0.030649673432975713\n",
      "Gradient Descent(267/299): loss=99940.38856432762, w0=-0.8913273103367076, w1=0.031032704688924637\n",
      "Gradient Descent(268/299): loss=99939.73610425278, w0=-0.8913957961221326, w1=0.031413133673144425\n",
      "Gradient Descent(269/299): loss=99939.09225461433, w0=-0.891463741466624, w1=0.03179097558520064\n",
      "Gradient Descent(270/299): loss=99938.45688951373, w0=-0.8915311508592614, w1=0.032166245605826184\n",
      "Gradient Descent(271/299): loss=99937.82988507193, w0=-0.8915980287445049, w1=0.03253895889475139\n",
      "Gradient Descent(272/299): loss=99937.21111939344, w0=-0.8916643795228073, w1=0.032909130588606665\n",
      "Gradient Descent(273/299): loss=99936.60047253163, w0=-0.891730207551214, w1=0.03327677579889614\n",
      "Gradient Descent(274/299): loss=99935.99782645432, w0=-0.8917955171439526, w1=0.033641909610040155\n",
      "Gradient Descent(275/299): loss=99935.40306501037, w0=-0.8918603125730109, w1=0.0340045470774848\n",
      "Gradient Descent(276/299): loss=99934.81607389648, w0=-0.8919245980687052, w1=0.03436470322587684\n",
      "Gradient Descent(277/299): loss=99934.23674062526, w0=-0.8919883778202367, w1=0.03472239304730237\n",
      "Gradient Descent(278/299): loss=99933.66495449329, w0=-0.8920516559762394, w1=0.035077631499587085\n",
      "Gradient Descent(279/299): loss=99933.10060655023, w0=-0.892114436645317, w1=0.03543043350465736\n",
      "Gradient Descent(280/299): loss=99932.54358956832, w0=-0.8921767238965699, w1=0.03578081394695948\n",
      "Gradient Descent(281/299): loss=99931.99379801257, w0=-0.8922385217601134, w1=0.03612878767193634\n",
      "Gradient Descent(282/299): loss=99931.45112801132, w0=-0.8922998342275862, w1=0.03647436948455973\n",
      "Gradient Descent(283/299): loss=99930.9154773279, w0=-0.8923606652526492, w1=0.03681757414791656\n",
      "Gradient Descent(284/299): loss=99930.38674533204, w0=-0.8924210187514766, w1=0.03715841638184801\n",
      "Gradient Descent(285/299): loss=99929.86483297253, w0=-0.8924808986032369, w1=0.0374969108616398\n",
      "Gradient Descent(286/299): loss=99929.34964275002, w0=-0.892540308650566, w1=0.03783307221676241\n",
      "Gradient Descent(287/299): loss=99928.84107869041, w0=-0.8925992527000324, w1=0.03816691502965984\n",
      "Gradient Descent(288/299): loss=99928.3390463188, w0=-0.8926577345225928, w1=0.038498453834585604\n",
      "Gradient Descent(289/299): loss=99927.84345263391, w0=-0.8927157578540412, w1=0.03882770311648483\n",
      "Gradient Descent(290/299): loss=99927.35420608296, w0=-0.8927733263954492, w1=0.039154677309920824\n",
      "Gradient Descent(291/299): loss=99926.87121653705, w0=-0.8928304438135983, w1=0.03947939079804544\n",
      "Gradient Descent(292/299): loss=99926.39439526695, w0=-0.8928871137414063, w1=0.03980185791161154\n",
      "Gradient Descent(293/299): loss=99925.92365491949, w0=-0.8929433397783441, w1=0.040122092928026834\n",
      "Gradient Descent(294/299): loss=99925.4589094941, w0=-0.8929991254908471, w1=0.04044011007044765\n",
      "Gradient Descent(295/299): loss=99925.00007432027, w0=-0.8930544744127188, w1=0.040755923506911665\n",
      "Gradient Descent(296/299): loss=99924.5470660346, w0=-0.8931093900455271, w1=0.041069547349508566\n",
      "Gradient Descent(297/299): loss=99924.09980255945, w0=-0.8931638758589948, w1=0.041380995653587556\n",
      "Gradient Descent(298/299): loss=99923.65820308094, w0=-0.8932179352913825, w1=0.04169028241700059\n",
      "Gradient Descent(299/299): loss=99923.22218802781, w0=-0.8932715717498655, w1=0.04199742157938059\n",
      "Gradient Descent(0/299): loss=138629.436111985, w0=-0.15727500000000003, w1=0.0053610009527342095\n",
      "Gradient Descent(1/299): loss=122137.773961997, w0=-0.2751412702325333, w1=0.008552410397489316\n",
      "Gradient Descent(2/299): loss=115377.1679186355, w0=-0.36811928564266444, w1=0.010571483211499802\n",
      "Gradient Descent(3/299): loss=111818.67361742721, w0=-0.44209743581533023, w1=0.009872301664331744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/299): loss=109610.10502859953, w0=-0.501828774601553, w1=0.007661342662352659\n",
      "Gradient Descent(5/299): loss=108130.88663086516, w0=-0.5505939829559122, w1=0.004521313146944431\n",
      "Gradient Descent(6/299): loss=107086.02145937573, w0=-0.5907656977113563, w1=0.0008692212597745438\n",
      "Gradient Descent(7/299): loss=106315.9973944444, w0=-0.6241191575832998, w1=-0.0030465850505832877\n",
      "Gradient Descent(8/299): loss=105727.68647437335, w0=-0.6520042918411324, w1=-0.007072741638045687\n",
      "Gradient Descent(9/299): loss=105263.80391110989, w0=-0.6754649876479493, w1=-0.01111509075029134\n",
      "Gradient Descent(10/299): loss=104887.70075016352, w0=-0.6953182642147416, w1=-0.015115378185189352\n",
      "Gradient Descent(11/299): loss=104575.1842699117, w0=-0.7122102771399673, w1=-0.019037485023576665\n",
      "Gradient Descent(12/299): loss=104309.86059709794, w0=-0.7266566060792511, w1=-0.02285910109306141\n",
      "Gradient Descent(13/299): loss=104080.36474134085, w0=-0.7390718684102504, w1=-0.026566664957175475\n",
      "Gradient Descent(14/299): loss=103878.65522111017, w0=-0.7497918264091807, w1=-0.030152248942131674\n",
      "Gradient Descent(15/299): loss=103698.9340297982, w0=-0.7590901092846214, w1=-0.033611613934062055\n",
      "Gradient Descent(16/299): loss=103536.9461869595, w0=-0.7671910032770878, w1=-0.03694297422436315\n",
      "Gradient Descent(17/299): loss=103389.5160655824, w0=-0.7742793295021961, w1=-0.04014619823089401\n",
      "Gradient Descent(18/299): loss=103254.23487391409, w0=-0.780508138438103, w1=-0.04322228033818472\n",
      "Gradient Descent(19/299): loss=103129.24658971003, w0=-0.7860047502715101, w1=-0.04617298411548305\n",
      "Gradient Descent(20/299): loss=103013.09916725119, w0=-0.7908755304084639, w1=-0.04900059605906834\n",
      "Gradient Descent(21/299): loss=102904.63971239302, w0=-0.795209689787005, w1=-0.0517077523822317\n",
      "Gradient Descent(22/299): loss=102802.93970428675, w0=-0.7990823276208967, w1=-0.054297315469072675\n",
      "Gradient Descent(23/299): loss=102707.24102241462, w0=-0.8025568815491885, w1=-0.05677228510855482\n",
      "Gradient Descent(24/299): loss=102616.91655543199, w0=-0.805687111254677, w1=-0.059135734747918944\n",
      "Gradient Descent(25/299): loss=102531.44114487414, w0=-0.8085187125855662, w1=-0.061390766095582576\n",
      "Gradient Descent(26/299): loss=102450.36993011188, w0=-0.8110906373726161, w1=-0.06354047729081615\n",
      "Gradient Descent(27/299): loss=102373.32204540045, w0=-0.813436177571546, w1=-0.06558794105124202\n",
      "Gradient Descent(28/299): loss=102299.96822307998, w0=-0.8155838597132293, w1=-0.06753619001837871\n",
      "Gradient Descent(29/299): loss=102230.02127320776, w0=-0.8175581859259816, w1=-0.06938820712294741\n",
      "Gradient Descent(30/299): loss=102163.2287001203, w0=-0.8193802502839391, w1=-0.07114691927689726\n",
      "Gradient Descent(31/299): loss=102099.36692058064, w0=-0.8210682533998336, w1=-0.0728151931080335\n",
      "Gradient Descent(32/299): loss=102038.2366927944, w0=-0.8226379336201993, w1=-0.07439583179948753\n",
      "Gradient Descent(33/299): loss=101979.6594685124, w0=-0.8241029295939427, w1=-0.0758915723849456\n",
      "Gradient Descent(34/299): loss=101923.47445380423, w0=-0.8254750861396639, w1=-0.07730508308774503\n",
      "Gradient Descent(35/299): loss=101869.53621625478, w0=-0.8267647130539901, w1=-0.07863896048924042\n",
      "Gradient Descent(36/299): loss=101817.71271320412, w0=-0.8279808046435788, w1=-0.07989572648573436\n",
      "Gradient Descent(37/299): loss=101767.8836416466, w0=-0.8291312262227374, w1=-0.08107782516092205\n",
      "Gradient Descent(38/299): loss=101719.9390292528, w0=-0.8302228725263605, w1=-0.08218761987228579\n",
      "Gradient Descent(39/299): loss=101673.77800131819, w0=-0.8312618019131907, w1=-0.08322739101747476\n",
      "Gradient Descent(40/299): loss=101629.3076739338, w0=-0.8322533493910149, w1=-0.0841993350747678\n",
      "Gradient Descent(41/299): loss=101586.44214230389, w0=-0.8332022209351978, w1=-0.08510556553583813\n",
      "Gradient Descent(42/299): loss=101545.10155544768, w0=-0.8341125713516336, w1=-0.08594811619680501\n",
      "Gradient Descent(43/299): loss=101505.21129070279, w0=-0.8349880680532851, w1=-0.08672894691153009\n",
      "Gradient Descent(44/299): loss=101466.70125552612, w0=-0.8358319434524902, w1=-0.08744995139792076\n",
      "Gradient Descent(45/299): loss=101429.50534214916, w0=-0.8366470389689876, w1=-0.08811296618692754\n",
      "Gradient Descent(46/299): loss=101393.56104160051, w0=-0.8374358436354665, w1=-0.08871977951796582\n",
      "Gradient Descent(47/299): loss=101358.80919634837, w0=-0.8382005297947636, w1=-0.08927213903942792\n",
      "Gradient Descent(48/299): loss=101325.19384936805, w0=-0.8389429875021277, w1=-0.08977175753633539\n",
      "Gradient Descent(49/299): loss=101292.66214138558, w0=-0.8396648582383034, w1=-0.09022031641291894\n",
      "Gradient Descent(50/299): loss=101261.16421702263, w0=-0.8403675677049597, w1=-0.09061946711292913\n",
      "Gradient Descent(51/299): loss=101230.6531169602, w0=-0.841052356997626, w1=-0.09097083094416145\n",
      "Gradient Descent(52/299): loss=101201.08464875707, w0=-0.8417203113488743, w1=-0.09127599786724969\n",
      "Gradient Descent(53/299): loss=101172.41723914165, w0=-0.8423723858004659, w1=-0.09153652476254243\n",
      "Gradient Descent(54/299): loss=101144.61177505083, w0=-0.843009427453991, w1=-0.0917539335724902\n",
      "Gradient Descent(55/299): loss=101117.63144114388, w0=-0.8436321942472139, w1=-0.09192970958622837\n",
      "Gradient Descent(56/299): loss=101091.44156001823, w0=-0.8442413704399635, w1=-0.09206530001962486\n",
      "Gradient Descent(57/299): loss=101066.00943933288, w0=-0.844837579146145, w1=-0.09216211295948273\n",
      "Gradient Descent(58/299): loss=101041.30422824211, w0=-0.8454213923230993, w1=-0.09222151668433667\n",
      "Gradient Descent(59/299): loss=101017.29678419702, w0=-0.8459933386446102, w1=-0.09224483934072958\n",
      "Gradient Descent(60/299): loss=100993.95955029123, w0=-0.8465539096600114, w1=-0.0922333689364081\n",
      "Gradient Descent(61/299): loss=100971.26644280177, w0=-0.8471035645966031, w1=-0.09218835360487729\n",
      "Gradient Descent(62/299): loss=100949.1927482962, w0=-0.8476427341087102, w1=-0.09211100209501617\n",
      "Gradient Descent(63/299): loss=100927.7150295584, w0=-0.8481718232224394, w1=-0.09200248444215152\n",
      "Gradient Descent(64/299): loss=100906.81103955687, w0=-0.8486912136751946, w1=-0.09186393278133734\n",
      "Gradient Descent(65/299): loss=100886.4596427024, w0=-0.8492012658055093, w1=-0.09169644226853403\n",
      "Gradient Descent(66/299): loss=100866.64074269336, w0=-0.849702320112382, w1=-0.0915010720803328\n",
      "Gradient Descent(67/299): loss=100847.33521630437, w0=-0.8501946985737775, w1=-0.09127884646752132\n",
      "Gradient Descent(68/299): loss=100828.52485253551, w0=-0.85067870579057, w1=-0.09103075584199273\n",
      "Gradient Descent(69/299): loss=100810.19229660027, w0=-0.8511546300040426, w1=-0.09075775788021616\n",
      "Gradient Descent(70/299): loss=100792.32099828511, w0=-0.8516227440211998, w1=-0.09046077862971522\n",
      "Gradient Descent(71/299): loss=100774.89516426128, w0=-0.852083306071768, w1=-0.09014071360776822\n",
      "Gradient Descent(72/299): loss=100757.89971397843, w0=-0.8525365606130967, w1=-0.08979842888389296\n",
      "Gradient Descent(73/299): loss=100741.32023880634, w0=-0.8529827390936393, w1=-0.0894347621396518\n",
      "Gradient Descent(74/299): loss=100725.14296412765, w0=-0.8534220606817629, w1=-0.08905052370095536\n",
      "Gradient Descent(75/299): loss=100709.35471411873, w0=-0.8538547329639277, w1=-0.0886464975393985\n",
      "Gradient Descent(76/299): loss=100693.94287897821, w0=-0.8542809526144627, w1=-0.08822344224026937\n",
      "Gradient Descent(77/299): loss=100678.89538439388, w0=-0.8547009060380054, w1=-0.08778209193576773\n",
      "Gradient Descent(78/299): loss=100664.20066305544, w0=-0.8551147699849843, w1=-0.08732315720268607\n",
      "Gradient Descent(79/299): loss=100649.84762804367, w0=-0.855522712140158, w1=-0.08684732592437272\n",
      "Gradient Descent(80/299): loss=100635.8256479413, w0=-0.8559248916840864, w1=-0.08635526411723825\n",
      "Gradient Descent(81/299): loss=100622.12452352754, w0=-0.8563214598274084, w1=-0.08584761672240382\n",
      "Gradient Descent(82/299): loss=100608.7344659316, w0=-0.856712560317897, w1=-0.08532500836334399\n",
      "Gradient Descent(83/299): loss=100595.64607613221, w0=-0.8570983299204005, w1=-0.08478804407056068\n",
      "Gradient Descent(84/299): loss=100582.85032570074, w0=-0.8574788988699403, w1=-0.0842373099744537\n",
      "Gradient Descent(85/299): loss=100570.33853869542, w0=-0.8578543912984015, w1=-0.08367337396763817\n",
      "Gradient Descent(86/299): loss=100558.10237462375, w0=-0.8582249256354039, w1=-0.08309678633800768\n",
      "Gradient Descent(87/299): loss=100546.13381239367, w0=-0.8585906149840808, w1=-0.0825080803738636\n",
      "Gradient Descent(88/299): loss=100534.42513518696, w0=-0.8589515674726079, w1=-0.08190777294243139\n",
      "Gradient Descent(89/299): loss=100522.96891618893, w0=-0.8593078865824186, w1=-0.08129636504306748\n",
      "Gradient Descent(90/299): loss=100511.75800511675, w0=-0.8596596714541154, w1=-0.08067434233643178\n",
      "Gradient Descent(91/299): loss=100500.7855154929, w0=-0.8600070171721372, w1=-0.08004217565086302\n",
      "Gradient Descent(92/299): loss=100490.0448126145, w0=-0.860350015029276, w1=-0.07940032146714922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(93/299): loss=100479.52950217409, w0=-0.8606887527721493, w1=-0.07874922238283705\n",
      "Gradient Descent(94/299): loss=100469.233419488, w0=-0.8610233148287384, w1=-0.0780893075571724\n",
      "Gradient Descent(95/299): loss=100459.1506192977, w0=-0.8613537825190901, w1=-0.07742099313771074\n",
      "Gradient Descent(96/299): loss=100449.27536610453, w0=-0.8616802342502562, w1=-0.07674468266958341\n",
      "Gradient Descent(97/299): loss=100439.60212500747, w0=-0.862002745696517, w1=-0.07606076748835203\n",
      "Gradient Descent(98/299): loss=100430.12555301347, w0=-0.8623213899658995, w1=-0.07536962709733146\n",
      "Gradient Descent(99/299): loss=100420.84049079043, w0=-0.8626362377539559, w1=-0.07467162953021104\n",
      "Gradient Descent(100/299): loss=100411.74195483835, w0=-0.8629473574857289, w1=-0.07396713169975472\n",
      "Gradient Descent(101/299): loss=100402.82513005324, w0=-0.8632548154467784, w1=-0.07325647973331341\n",
      "Gradient Descent(102/299): loss=100394.08536266122, w0=-0.8635586759040994, w1=-0.07254000929583848\n",
      "Gradient Descent(103/299): loss=100385.51815350176, w0=-0.8638590012177112, w1=-0.07181804590104186\n",
      "Gradient Descent(104/299): loss=100377.11915164013, w0=-0.8641558519436494, w1=-0.07109090521130838\n",
      "Gradient Descent(105/299): loss=100368.88414829058, w0=-0.864449286929045, w1=-0.07035889332692705\n",
      "Gradient Descent(106/299): loss=100360.80907103274, w0=-0.8647393633999294, w1=-0.06962230706517242\n",
      "Gradient Descent(107/299): loss=100352.88997830564, w0=-0.8650261370423579, w1=-0.06888143422973327\n",
      "Gradient Descent(108/299): loss=100345.12305416337, w0=-0.865309662077403, w1=-0.06813655387095376\n",
      "Gradient Descent(109/299): loss=100337.50460327888, w0=-0.865589991330527, w1=-0.06738793653732288\n",
      "Gradient Descent(110/299): loss=100330.03104618189, w0=-0.865867176295804, w1=-0.06663584451862022\n",
      "Gradient Descent(111/299): loss=100322.69891471913, w0=-0.8661412671954271, w1=-0.06588053208110005\n",
      "Gradient Descent(112/299): loss=100315.5048477246, w0=-0.8664123130348991, w1=-0.06512224569507183\n",
      "Gradient Descent(113/299): loss=100308.44558688885, w0=-0.8666803616542754, w1=-0.06436122425521286\n",
      "Gradient Descent(114/299): loss=100301.51797281695, w0=-0.8669454597757957, w1=-0.0635976992939276\n",
      "Gradient Descent(115/299): loss=100294.71894126572, w0=-0.8672076530482139, w1=-0.06283189518804945\n",
      "Gradient Descent(116/299): loss=100288.04551955021, w0=-0.8674669860881099, w1=-0.06206402935916237\n",
      "Gradient Descent(117/299): loss=100281.49482311108, w0=-0.8677235025184418, w1=-0.06129431246780317\n",
      "Gradient Descent(118/299): loss=100275.0640522354, w0=-0.8679772450045755, w1=-0.06052294860179008\n",
      "Gradient Descent(119/299): loss=100268.75048892197, w0=-0.8682282552880078, w1=-0.05975013545890868\n",
      "Gradient Descent(120/299): loss=100262.5514938844, w0=-0.8684765742179797, w1=-0.058976064524173054\n",
      "Gradient Descent(121/299): loss=100256.46450368539, w0=-0.86872224178116, w1=-0.05820092124186789\n",
      "Gradient Descent(122/299): loss=100250.48702799538, w0=-0.8689652971295626, w1=-0.057424885182565645\n",
      "Gradient Descent(123/299): loss=100244.61664696885, w0=-0.8692057786068472, w1=-0.05664813020530259\n",
      "Gradient Descent(124/299): loss=100238.85100873408, w0=-0.8694437237731372, w1=-0.05587082461508751\n",
      "Gradient Descent(125/299): loss=100233.18782698897, w0=-0.8696791694284796, w1=-0.05509313131590795\n",
      "Gradient Descent(126/299): loss=100227.62487869896, w0=-0.8699121516350585, w1=-0.05431520795939028\n",
      "Gradient Descent(127/299): loss=100222.16000189197, w0=-0.8701427057382636, w1=-0.0535372070892624\n",
      "Gradient Descent(128/299): loss=100216.7910935455, w0=-0.8703708663867064, w1=-0.0527592762817599\n",
      "Gradient Descent(129/299): loss=100211.51610756112, w0=-0.8705966675512677, w1=-0.05198155828211074\n",
      "Gradient Descent(130/299): loss=100206.33305282315, w0=-0.8708201425432538, w1=-0.05120419113722614\n",
      "Gradient Descent(131/299): loss=100201.23999133689, w0=-0.8710413240317281, w1=-0.050427308324720366\n",
      "Gradient Descent(132/299): loss=100196.23503644251, w0=-0.8712602440600838, w1=-0.049651038878376054\n",
      "Gradient Descent(133/299): loss=100191.31635110185, w0=-0.8714769340619131, w1=-0.048875507510166845\n",
      "Gradient Descent(134/299): loss=100186.48214625382, w0=-0.8716914248762234, w1=-0.04810083472894431\n",
      "Gradient Descent(135/299): loss=100181.73067923594, w0=-0.8719037467620516, w1=-0.04732713695589153\n",
      "Gradient Descent(136/299): loss=100177.0602522686, w0=-0.8721139294125138, w1=-0.0465545266368416\n",
      "Gradient Descent(137/299): loss=100172.46921099935, w0=-0.8723220019683333, w1=-0.04578311235155538\n",
      "Gradient Descent(138/299): loss=100167.95594310432, w0=-0.8725279930308806, w1=-0.04501299892004904\n",
      "Gradient Descent(139/299): loss=100163.51887694436, w0=-0.8727319306747566, w1=-0.044244287506058536\n",
      "Gradient Descent(140/299): loss=100159.1564802735, w0=-0.8729338424599499, w1=-0.04347707571772468\n",
      "Gradient Descent(141/299): loss=100154.8672589969, w0=-0.8731337554435922, w1=-0.04271145770557972\n",
      "Gradient Descent(142/299): loss=100150.64975597686, w0=-0.8733316961913393, w1=-0.041947524257912934\n",
      "Gradient Descent(143/299): loss=100146.50254988414, w0=-0.8735276907883962, w1=-0.04118536289359046\n",
      "Gradient Descent(144/299): loss=100142.42425409253, w0=-0.8737217648502084, w1=-0.04042505795240151\n",
      "Gradient Descent(145/299): loss=100138.41351561589, w0=-0.873913943532837, w1=-0.03966669068300091\n",
      "Gradient Descent(146/299): loss=100134.46901408385, w0=-0.874104251543035, w1=-0.03891033932851536\n",
      "Gradient Descent(147/299): loss=100130.58946075643, w0=-0.8742927131480382, w1=-0.0381560792098788\n",
      "Gradient Descent(148/299): loss=100126.77359757459, w0=-0.8744793521850879, w1=-0.03740398280695967\n",
      "Gradient Descent(149/299): loss=100123.02019624619, w0=-0.8746641920706942, w1=-0.03665411983754126\n",
      "Gradient Descent(150/299): loss=100119.3280573646, w0=-0.874847255809656, w1=-0.03590655733421424\n",
      "Gradient Descent(151/299): loss=100115.69600955988, w0=-0.8750285660038443, w1=-0.035161359719238346\n",
      "Gradient Descent(152/299): loss=100112.12290868015, w0=-0.875208144860762, w1=-0.03441858887742876\n",
      "Gradient Descent(153/299): loss=100108.60763700263, w0=-0.8753860142018886, w1=-0.03367830422712076\n",
      "Gradient Descent(154/299): loss=100105.14910247232, w0=-0.8755621954708165, w1=-0.0329405627892648\n",
      "Gradient Descent(155/299): loss=100101.74623796824, w0=-0.8757367097411899, w1=-0.0322054192547023\n",
      "Gradient Descent(156/299): loss=100098.3980005946, w0=-0.8759095777244505, w1=-0.03147292604967113\n",
      "Gradient Descent(157/299): loss=100095.10337099759, w0=-0.8760808197773996, w1=-0.030743133399588323\n",
      "Gradient Descent(158/299): loss=100091.86135270534, w0=-0.8762504559095803, w1=-0.030016089391155944\n",
      "Gradient Descent(159/299): loss=100088.67097149082, w0=-0.876418505790488, w1=-0.02929184003283504\n",
      "Gradient Descent(160/299): loss=100085.53127475645, w0=-0.8765849887566136, w1=-0.028570429313730775\n",
      "Gradient Descent(161/299): loss=100082.44133094003, w0=-0.8767499238183243, w1=-0.027851899260931266\n",
      "Gradient Descent(162/299): loss=100079.4002289402, w0=-0.8769133296665875, w1=-0.027136289995340786\n",
      "Gradient Descent(163/299): loss=100076.40707756177, w0=-0.8770752246795429, w1=-0.026423639786047264\n",
      "Gradient Descent(164/299): loss=100073.46100497912, w0=-0.8772356269289255, w1=-0.025713985103262744\n",
      "Gradient Descent(165/299): loss=100070.56115821793, w0=-0.8773945541863452, w1=-0.025007360669874197\n",
      "Gradient Descent(166/299): loss=100067.70670265326, w0=-0.8775520239294261, w1=-0.024303799511641504\n",
      "Gradient Descent(167/299): loss=100064.89682152506, w0=-0.87770805334781, w1=-0.02360333300607776\n",
      "Gradient Descent(168/299): loss=100062.13071546856, w0=-0.8778626593490259, w1=-0.02290599093004667\n",
      "Gradient Descent(169/299): loss=100059.40760206047, w0=-0.8780158585642308, w1=-0.02221180150611052\n",
      "Gradient Descent(170/299): loss=100056.72671537934, w0=-0.8781676673538246, w1=-0.02152079144766117\n",
      "Gradient Descent(171/299): loss=100054.0873055803, w0=-0.8783181018129406, w1=-0.020832986002866153\n",
      "Gradient Descent(172/299): loss=100051.48863848319, w0=-0.8784671777768172, w1=-0.02014840899746043\n",
      "Gradient Descent(173/299): loss=100048.92999517362, w0=-0.8786149108260511, w1=-0.019467082876413976\n",
      "Gradient Descent(174/299): loss=100046.41067161667, w0=-0.8787613162917369, w1=-0.018789028744504443\n",
      "Gradient Descent(175/299): loss=100043.92997828248, w0=-0.8789064092604937, w1=-0.018114266405823205\n",
      "Gradient Descent(176/299): loss=100041.48723978354, w0=-0.8790502045793833, w1=-0.017442814402242525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(177/299): loss=100039.08179452323, w0=-0.8791927168607209, w1=-0.01677469005087076\n",
      "Gradient Descent(178/299): loss=100036.71299435472, w0=-0.8793339604867814, w1=-0.01610990948052175\n",
      "Gradient Descent(179/299): loss=100034.38020425073, w0=-0.8794739496144037, w1=-0.015448487667223853\n",
      "Gradient Descent(180/299): loss=100032.08280198305, w0=-0.8796126981794951, w1=-0.014790438468793668\n",
      "Gradient Descent(181/299): loss=100029.8201778114, w0=-0.8797502199014374, w1=-0.014135774658498194\n",
      "Gradient Descent(182/299): loss=100027.59173418219, w0=-0.8798865282873987, w1=-0.013484507957829264\n",
      "Gradient Descent(183/299): loss=100025.39688543558, w0=-0.8800216366365499, w1=-0.01283664906841311\n",
      "Gradient Descent(184/299): loss=100023.23505752152, w0=-0.8801555580441917, w1=-0.012192207703077149\n",
      "Gradient Descent(185/299): loss=100021.10568772405, w0=-0.8802883054057904, w1=-0.011551192616095838\n",
      "Gradient Descent(186/299): loss=100019.0082243935, w0=-0.8804198914209275, w1=-0.010913611632636735\n",
      "Gradient Descent(187/299): loss=100016.94212668664, w0=-0.8805503285971628, w1=-0.010279471677427291\n",
      "Gradient Descent(188/299): loss=100014.90686431405, w0=-0.880679629253814, w1=-0.009648778802662307\n",
      "Gradient Descent(189/299): loss=100012.90191729486, w0=-0.8808078055256544, w1=-0.009021538215171793\n",
      "Gradient Descent(190/299): loss=100010.9267757185, w0=-0.8809348693665296, w1=-0.008397754302867903\n",
      "Gradient Descent(191/299): loss=100008.98093951296, w0=-0.8810608325528974, w1=-0.007777430660489857\n",
      "Gradient Descent(192/299): loss=100007.06391821968, w0=-0.8811857066872884, w1=-0.00716057011466447\n",
      "Gradient Descent(193/299): loss=100005.17523077491, w0=-0.8813095032016938, w1=-0.006547174748300082\n",
      "Gradient Descent(194/299): loss=100003.31440529667, w0=-0.8814322333608781, w1=-0.005937245924330864\n",
      "Gradient Descent(195/299): loss=100001.48097887804, w0=-0.8815539082656203, w1=-0.005330784308828276\n",
      "Gradient Descent(196/299): loss=99999.6744973857, w0=-0.8816745388558846, w1=-0.004727789893495699\n",
      "Gradient Descent(197/299): loss=99997.89451526434, w0=-0.8817941359139225, w1=-0.004128262017562251\n",
      "Gradient Descent(198/299): loss=99996.14059534593, w0=-0.8819127100673069, w1=-0.003532199389091032\n",
      "Gradient Descent(199/299): loss=99994.41230866485, w0=-0.8820302717919004, w1=-0.0029396001057168976\n",
      "Gradient Descent(200/299): loss=99992.70923427683, w0=-0.8821468314147596, w1=-0.0023504616748282236\n",
      "Gradient Descent(201/299): loss=99991.03095908382, w0=-0.8822623991169755, w1=-0.0017647810332070317\n",
      "Gradient Descent(202/299): loss=99989.37707766281, w0=-0.8823769849364522, w1=-0.0011825545661413705\n",
      "Gradient Descent(203/299): loss=99987.74719209949, w0=-0.882490598770626, w1=-0.0006037781260232451\n",
      "Gradient Descent(204/299): loss=99986.14091182593, w0=-0.882603250379124, w1=-2.844705044551031e-05\n",
      "Gradient Descent(205/299): loss=99984.5578534629, w0=-0.8827149493863663, w1=0.0005434438201896053\n",
      "Gradient Descent(206/299): loss=99982.99764066587, w0=-0.8828257052841106, w1=0.0011119001255378252\n",
      "Gradient Descent(207/299): loss=99981.45990397512, w0=-0.882935527433942, w1=0.0016769279686437212\n",
      "Gradient Descent(208/299): loss=99979.94428066973, w0=-0.883044425069708, w1=0.002238533899723467\n",
      "Gradient Descent(209/299): loss=99978.45041462545, w0=-0.883152407299902, w1=0.0027967249003878225\n",
      "Gradient Descent(210/299): loss=99976.97795617537, w0=-0.883259483109993, w1=0.0033515083682939158\n",
      "Gradient Descent(211/299): loss=99975.52656197532, w0=-0.8833656613647071, w1=0.003902892102215174\n",
      "Gradient Descent(212/299): loss=99974.09589487166, w0=-0.883470950810257, w1=0.004450884287518433\n",
      "Gradient Descent(213/299): loss=99972.68562377257, w0=-0.8835753600765248, w1=0.004995493482037907\n",
      "Gradient Descent(214/299): loss=99971.29542352291, w0=-0.8836788976791972, w1=0.0055367286023358065\n",
      "Gradient Descent(215/299): loss=99969.9249747817, w0=-0.8837815720218536, w1=0.00607459891033977\n",
      "Gradient Descent(216/299): loss=99968.57396390282, w0=-0.88388339139801, w1=0.006609114000347312\n",
      "Gradient Descent(217/299): loss=99967.24208281856, w0=-0.8839843639931177, w1=0.0071402837863881125\n",
      "Gradient Descent(218/299): loss=99965.92902892604, w0=-0.88408449788652, w1=0.007668118489934733\n",
      "Gradient Descent(219/299): loss=99964.63450497622, w0=-0.8841838010533647, w1=0.008192628627953023\n",
      "Gradient Descent(220/299): loss=99963.35821896582, w0=-0.8842822813664774, w1=0.008713825001283248\n",
      "Gradient Descent(221/299): loss=99962.0998840316, w0=-0.8843799465981924, w1=0.009231718683343817\n",
      "Gradient Descent(222/299): loss=99960.8592183472, w0=-0.8844768044221462, w1=0.009746321009148995\n",
      "Gradient Descent(223/299): loss=99959.63594502248, w0=-0.88457286241503, w1=0.010257643564632716\n",
      "Gradient Descent(224/299): loss=99958.42979200512, w0=-0.8846681280583063, w1=0.010765698176270551\n",
      "Gradient Descent(225/299): loss=99957.24049198469, w0=-0.8847626087398881, w1=0.011270496900992253\n",
      "Gradient Descent(226/299): loss=99956.06778229872, w0=-0.8848563117557812, w1=0.011772052016377254\n",
      "Gradient Descent(227/299): loss=99954.91140484129, w0=-0.8849492443116925, w1=0.012270376011126119\n",
      "Gradient Descent(228/299): loss=99953.77110597317, w0=-0.8850414135246032, w1=0.012765481575800477\n",
      "Gradient Descent(229/299): loss=99952.64663643461, w0=-0.8851328264243079, w1=0.01325738159382482\n",
      "Gradient Descent(230/299): loss=99951.5377512599, w0=-0.8852234899549218, w1=0.013746089132743269\n",
      "Gradient Descent(231/299): loss=99950.44420969361, w0=-0.885313410976355, w1=0.014231617435724833\n",
      "Gradient Descent(232/299): loss=99949.36577510921, w0=-0.8854025962657551, w1=0.014713979913310615\n",
      "Gradient Descent(233/299): loss=99948.3022149291, w0=-0.8854910525189204, w1=0.015193190135396896\n",
      "Gradient Descent(234/299): loss=99947.25330054676, w0=-0.885578786351681, w1=0.0156692618234478\n",
      "Gradient Descent(235/299): loss=99946.21880725029, w0=-0.8856658043012529, w1=0.016142208842931816\n",
      "Gradient Descent(236/299): loss=99945.19851414801, w0=-0.8857521128275613, w1=0.016612045195976193\n",
      "Gradient Descent(237/299): loss=99944.19220409553, w0=-0.8858377183145374, w1=0.017078785014233804\n",
      "Gradient Descent(238/299): loss=99943.19966362431, w0=-0.885922627071387, w1=0.017542442551956663\n",
      "Gradient Descent(239/299): loss=99942.220682872, w0=-0.8860068453338327, w1=0.018003032179271034\n",
      "Gradient Descent(240/299): loss=99941.25505551432, w0=-0.8860903792653293, w1=0.018460568375648745\n",
      "Gradient Descent(241/299): loss=99940.30257869807, w0=-0.886173234958255, w1=0.018915065723569637\n",
      "Gradient Descent(242/299): loss=99939.36305297616, w0=-0.886255418435076, w1=0.019366538902370247\n",
      "Gradient Descent(243/299): loss=99938.43628224343, w0=-0.886336935649488, w1=0.019815002682273746\n",
      "Gradient Descent(244/299): loss=99937.52207367441, w0=-0.8864177924875328, w1=0.02026047191859653\n",
      "Gradient Descent(245/299): loss=99936.62023766185, w0=-0.8864979947686928, w1=0.0207029615461269\n",
      "Gradient Descent(246/299): loss=99935.73058775716, w0=-0.8865775482469616, w1=0.021142486573671203\n",
      "Gradient Descent(247/299): loss=99934.85294061167, w0=-0.8866564586118927, w1=0.02157906207876314\n",
      "Gradient Descent(248/299): loss=99933.98711591933, w0=-0.8867347314896271, w1=0.022012703202531986\n",
      "Gradient Descent(249/299): loss=99933.13293636042, w0=-0.8868123724438987, w1=0.022443425144725578\n",
      "Gradient Descent(250/299): loss=99932.29022754682, w0=-0.8868893869770192, w1=0.022871243158884023\n",
      "Gradient Descent(251/299): loss=99931.45881796806, w0=-0.8869657805308439, w1=0.02329617254766006\n",
      "Gradient Descent(252/299): loss=99930.6385389388, w0=-0.8870415584877153, w1=0.02371822865828236\n",
      "Gradient Descent(253/299): loss=99929.82922454676, w0=-0.8871167261713897, w1=0.024137426878158053\n",
      "Gradient Descent(254/299): loss=99929.03071160291, w0=-0.8871912888479436, w1=0.02455378263061063\n",
      "Gradient Descent(255/299): loss=99928.24283959134, w0=-0.8872652517266618, w1=0.02496731137074983\n",
      "Gradient Descent(256/299): loss=99927.46545062136, w0=-0.8873386199609077, w1=0.025378028581469956\n",
      "Gradient Descent(257/299): loss=99926.6983893794, w0=-0.8874113986489754, w1=0.025785949769573262\n",
      "Gradient Descent(258/299): loss=99925.9415030831, w0=-0.8874835928349257, w1=0.026191090462015107\n",
      "Gradient Descent(259/299): loss=99925.19464143542, w0=-0.8875552075094036, w1=0.026593466202267622\n",
      "Gradient Descent(260/299): loss=99924.45765658008, w0=-0.887626247610441, w1=0.026993092546798738\n",
      "Gradient Descent(261/299): loss=99923.7304030579, w0=-0.8876967180242417, w1=0.02738998506166365\n",
      "Gradient Descent(262/299): loss=99923.01273776381, w0=-0.8877666235859522, w1=0.02778415931920553\n",
      "Gradient Descent(263/299): loss=99922.30451990508, w0=-0.887835969080416, w1=0.028175630894862795\n",
      "Gradient Descent(264/299): loss=99921.6056109599, w0=-0.8879047592429133, w1=0.028564415364079908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(265/299): loss=99920.91587463736, w0=-0.8879729987598864, w1=0.028950528299319038\n",
      "Gradient Descent(266/299): loss=99920.23517683754, w0=-0.8880406922696498, w1=0.02933398526716989\n",
      "Gradient Descent(267/299): loss=99919.56338561325, w0=-0.8881078443630869, w1=0.02971480182555518\n",
      "Gradient Descent(268/299): loss=99918.90037113165, w0=-0.8881744595843335, w1=0.030092993521028844\n",
      "Gradient Descent(269/299): loss=99918.2460056372, w0=-0.888240542431447, w1=0.03046857588616497\n",
      "Gradient Descent(270/299): loss=99917.60016341516, w0=-0.8883060973570627, w1=0.030841564437034612\n",
      "Gradient Descent(271/299): loss=99916.9627207559, w0=-0.8883711287690378, w1=0.031211974670768437\n",
      "Gradient Descent(272/299): loss=99916.3335559195, w0=-0.8884356410310823, w1=0.03157982206320262\n",
      "Gradient Descent(273/299): loss=99915.71254910185, w0=-0.8884996384633783, w1=0.03194512206660588\n",
      "Gradient Descent(274/299): loss=99915.09958240043, w0=-0.8885631253431868, w1=0.032307890107485526\n",
      "Gradient Descent(275/299): loss=99914.49453978158, w0=-0.8886261059054431, w1=0.03266814158447024\n",
      "Gradient Descent(276/299): loss=99913.8973070479, w0=-0.8886885843433407, w1=0.03302589186626754\n",
      "Gradient Descent(277/299): loss=99913.30777180658, w0=-0.8887505648089045, w1=0.033381156289693945\n",
      "Gradient Descent(278/299): loss=99912.72582343794, w0=-0.888812051413552, w1=0.03373395015777587\n",
      "Gradient Descent(279/299): loss=99912.15135306513, w0=-0.8888730482286453, w1=0.034084288737919216\n",
      "Gradient Descent(280/299): loss=99911.584253524, w0=-0.8889335592860316, w1=0.03443218726014591\n",
      "Gradient Descent(281/299): loss=99911.02441933358, w0=-0.8889935885785736, w1=0.034777660915395484\n",
      "Gradient Descent(282/299): loss=99910.47174666726, w0=-0.8890531400606708, w1=0.035120724853889874\n",
      "Gradient Descent(283/299): loss=99909.92613332452, w0=-0.8891122176487704, w1=0.03546139418355981\n",
      "Gradient Descent(284/299): loss=99909.38747870307, w0=-0.8891708252218681, w1=0.035799683968531046\n",
      "Gradient Descent(285/299): loss=99908.85568377143, w0=-0.8892289666220011, w1=0.03613560922766865\n",
      "Gradient Descent(286/299): loss=99908.33065104258, w0=-0.8892866456547311, w1=0.036469184933177995\n",
      "Gradient Descent(287/299): loss=99907.81228454722, w0=-0.889343866089618, w1=0.03680042600926073\n",
      "Gradient Descent(288/299): loss=99907.30048980823, w0=-0.8894006316606858, w1=0.037129347330824077\n",
      "Gradient Descent(289/299): loss=99906.79517381547, w0=-0.8894569460668799, w1=0.0374559637222423\n",
      "Gradient Descent(290/299): loss=99906.29624500085, w0=-0.889512812972515, w1=0.03778028995616861\n",
      "Gradient Descent(291/299): loss=99905.80361321395, w0=-0.8895682360077167, w1=0.03810234075239613\n",
      "Gradient Descent(292/299): loss=99905.31718969825, w0=-0.8896232187688534, w1=0.03842213077676669\n",
      "Gradient Descent(293/299): loss=99904.83688706752, w0=-0.8896777648189615, w1=0.03873967464012592\n",
      "Gradient Descent(294/299): loss=99904.36261928288, w0=-0.8897318776881629, w1=0.039054986897323445\n",
      "Gradient Descent(295/299): loss=99903.89430163021, w0=-0.8897855608740743, w1=0.03936808204625693\n",
      "Gradient Descent(296/299): loss=99903.43185069795, w0=-0.8898388178422105, w1=0.039678974526958656\n",
      "Gradient Descent(297/299): loss=99902.97518435528, w0=-0.8898916520263793, w1=0.03998767872072339\n",
      "Gradient Descent(298/299): loss=99902.52422173086, w0=-0.8899440668290706, w1=0.04029420894927648\n",
      "Gradient Descent(299/299): loss=99902.0788831916, w0=-0.8899960656218374, w1=0.040598579473980975\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(stand_x, y, 0.8, myseed=1)\n",
    "losses, ws = logistic_regression(y_tr, x_tr , np.zeros(stand_x.shape[1]),300, 0.000005)\n",
    "\n",
    "for i in range(1, 5):\n",
    "    x_tr, x_te, y_tr, y_te = split_data(stand_x, y, 0.8, myseed=i)\n",
    "    losses, w = logistic_regression(y_tr, x_tr , np.zeros(stand_x.shape[1]),300, 0.000005)\n",
    "    ws += w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ws\n",
    "\n",
    "# w = ws/5\n",
    "\n",
    "# y, x, ind = load_csv_data('higgs-data/test.csv')\n",
    "# stand_x = standardize(x, True)\n",
    "\n",
    "# y_pred = predict_labels(w, stand_x)\n",
    "\n",
    "# create_csv_submission(ind, y_pred, 'prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
